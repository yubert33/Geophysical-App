import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats
from io import BytesIO
import re
from datetime import datetime
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
import plotly.graph_objects as go
import plotly.express as px
import joblib
import warnings
import tempfile
import os
warnings.filterwarnings('ignore')

# Verificar disponibilidad de lasio
try:
    import lasio
    LASIO_AVAILABLE = True
except ImportError:
    LASIO_AVAILABLE = False

# ConfiguraciÃ³n de la pÃ¡gina
st.set_page_config(layout="wide", page_title="Sistema Avanzado de AnÃ¡lisis PetrofÃ­sico")
st.title("ðŸ›¢ï¸ Sistema Avanzado de AnÃ¡lisis PetrofÃ­sico")
st.write("**Herramienta integral para visualizaciÃ³n, anÃ¡lisis e interpretaciÃ³n de registros de pozo**")

# Mostrar advertencia si lasio no estÃ¡ disponible
if not LASIO_AVAILABLE:
    st.warning("""
    âš ï¸ **Para mejor procesamiento de archivos LAS, instala lasio:**
    ```bash
    pip install lasio
    ```
    """)
    # ================================
# ðŸ“ FUNCIONES MEJORADAS CON LASIO PARA ARCHIVOS LAS
# ================================
def procesar_las_con_lasio(archivo):
    """
    FunciÃ³n mejorada para procesar archivos LAS usando la librerÃ­a lasio
    """
    try:
        # Crear archivo temporal
        with tempfile.NamedTemporaryFile(delete=False, suffix='.las') as temp_file:
            temp_file.write(archivo.getvalue())
            temp_path = temp_file.name
        
        try:
            import lasio
            st.info("ðŸ” Procesando archivo LAS con lasio...")
            
            # Leer archivo LAS con lasio
            las = lasio.read(temp_path)
            
            # Convertir a DataFrame
            df = las.df()
            df.reset_index(inplace=True)  # La profundidad viene como Ã­ndice
            
            # Obtener informaciÃ³n de las curvas (CORREGIDO)
            curve_info = []
            for mnemonic, curve in las.curves.items():
                curve_info.append({
                    'MNEMONIC': mnemonic,
                    'UNIT': curve.unit if curve.unit else '',
                    'DESCRIPTION': curve.descr if curve.descr else '',
                    'VALUE': curve.value if curve.value else '',
                    'API_CODE': curve.API_code if hasattr(curve, 'API_code') else ''  # CORREGIDO
                })
            
            # Obtener metadatos del pozo
            well_info = []
            if hasattr(las, 'well') and las.well:
                for mnemonic, item in las.well.items():
                    well_info.append({
                        'MNEMONIC': mnemonic,
                        'UNIT': item.unit if hasattr(item, 'unit') else '',
                        'VALUE': item.value if hasattr(item, 'value') else '',
                        'DESCRIPTION': item.descr if hasattr(item, 'descr') else ''
                    })
            
            # Obtener informaciÃ³n de parÃ¡metros
            parameter_info = []
            if hasattr(las, 'params') and las.params:
                for mnemonic, param in las.params.items():
                    parameter_info.append({
                        'MNEMONIC': mnemonic,
                        'UNIT': param.unit if hasattr(param, 'unit') else '',
                        'VALUE': param.value if hasattr(param, 'value') else '',
                        'DESCRIPTION': param.descr if hasattr(param, 'descr') else ''
                    })
            
            sections = {
                'curve': curve_info,
                'well': well_info,
                'parameter': parameter_info,
                'version': getattr(las, 'version', [])
            }
            
            # Limpiar archivo temporal
            os.unlink(temp_path)
            
            st.success(f"âœ… Archivo LAS procesado con lasio: {len(df)} filas, {len(df.columns)} columnas")
            return df, sections
            
        except ImportError:
            st.warning("âš ï¸ Lasio no estÃ¡ instalado. Usando procesamiento manual...")
            os.unlink(temp_path)
            return procesar_las_manual(archivo)
            
    except Exception as e:
        st.error(f"âŒ Error procesando LAS con lasio: {str(e)}")
        import traceback
        st.error(f"Detalles: {traceback.format_exc()}")
        # Fallback al mÃ©todo manual
        return procesar_las_manual(archivo)

def procesar_las_manual(archivo):
    """
    FunciÃ³n de respaldo para procesar archivos LAS manualmente
    """
    try:
        # Leer todo el contenido
        contenido = archivo.getvalue().decode('utf-8', errors='ignore')
        lineas = contenido.split('\n')
        
        st.info("ðŸ” Procesando archivo LAS (enfoque manual)...")
        
        # Buscar la secciÃ³n de datos (~A)
        inicio_datos = None
        for i, linea in enumerate(lineas):
            if linea.strip().startswith('~A'):
                inicio_datos = i + 1
                break
        
        if inicio_datos is None:
            st.error("âŒ No se encontrÃ³ la secciÃ³n ~A en el archivo")
            return None, None
        
        # Extraer datos despuÃ©s de ~A
        datos = []
        for i in range(inicio_datos, len(lineas)):
            linea = lineas[i].strip()
            if not linea or linea.startswith('#'):
                continue
                
            # Dividir por espacios/tabs y filtrar elementos vacÃ­os
            partes = re.split(r'\s+', linea)
            partes = [p for p in partes if p]
            
            # Verificar si tiene nÃºmeros
            if partes and any(parte.replace('.', '').replace('-', '').isdigit() for parte in partes):
                datos.append(partes)
        
        if not datos:
            st.error("âŒ No se encontraron datos numÃ©ricos despuÃ©s de ~A")
            return None, None
        
        st.info(f"âœ… {len(datos)} filas de datos encontradas")
        
        # USAR NOMBRES DE COLUMNAS MANUALES
        column_names = ['DEPTH', 'CALI', 'SP', 'ILM', 'ILD', 'LAT', 'CILD']
        
        # Verificar que tenemos suficientes columnas
        num_columnas_datos = len(datos[0])
        if num_columnas_datos != len(column_names):
            st.warning(f"âš ï¸ NÃºmero de columnas esperado: {len(column_names)}, encontrado: {num_columnas_datos}")
            # Ajustar nombres si es necesario
            if num_columnas_datos > len(column_names):
                column_names.extend([f'COL_{i}' for i in range(len(column_names), num_columnas_datos)])
            else:
                column_names = column_names[:num_columnas_datos]
        
        # Crear DataFrame
        data_records = []
        for fila in datos:
            processed_values = []
            for val in fila:
                try:
                    processed_values.append(float(val))
                except ValueError:
                    processed_values.append(val)
            data_records.append(processed_values)
        
        df = pd.DataFrame(data_records, columns=column_names)
        
        # Reemplazar valores NULL
        null_values = [-999.25, -999.250000, -999.2500]
        for null_val in null_values:
            df.replace(null_val, np.nan, inplace=True)
        
        # Extraer informaciÃ³n de secciones para metadata
        sections = {}
        current_section = None
        
        for linea in lineas:
            linea_limpia = linea.strip()
            
            if linea_limpia.startswith('~'):
                if 'V' in linea_limpia:
                    current_section = 'version'
                elif 'W' in linea_limpia:
                    current_section = 'well'
                elif 'C' in linea_limpia:
                    current_section = 'curve'
                elif 'P' in linea_limpia:
                    current_section = 'parameter'
                else:
                    current_section = 'other'
                
                if current_section not in sections:
                    sections[current_section] = []
                continue
            
            if current_section and linea_limpia and not linea_limpia.startswith('#'):
                sections[current_section].append(linea_limpia)
        
        st.success(f"âœ… Archivo LAS procesado: {len(df)} filas, {len(df.columns)} columnas")
        return df, sections
        
    except Exception as e:
        st.error(f"âŒ Error procesando LAS manualmente: {str(e)}")
        return None, None
def descargar_las_como_excel(df, sections, nombre_archivo="datos_las_convertidos.xlsx"):
    """Crea archivo Excel descargable con informaciÃ³n completa de curvas usando lasio"""
    try:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # ===== HOJA 1: DATOS PRINCIPALES =====
            df.to_excel(writer, index=False, sheet_name='DATOS')
            
            # ===== HOJA 2: METADATOS Y INFORMACIÃ“N DE CURVAS =====
            metadata_rows = []
            
            # Agregar informaciÃ³n de la secciÃ³n de curvas primero (mÃ¡s importante)
            if 'curve' in sections and sections['curve']:
                metadata_rows.append("=== CURVE INFORMATION SECTION ===")
                metadata_rows.append("MNEMONIC\tUNIT\tDESCRIPTION\tVALUE\tAPI_CODE")
                metadata_rows.append("--------\t----\t-----------\t-----\t--------")
                
                # Verificar si es una lista de diccionarios (lasio) o strings (manual)
                if isinstance(sections['curve'][0], dict):
                    for curve_info in sections['curve']:
                        # CORREGIDO: Usar get() para evitar errores si falta algÃºn campo
                        row = f"{curve_info.get('MNEMONIC', '')}\t{curve_info.get('UNIT', '')}\t{curve_info.get('DESCRIPTION', '')}\t{curve_info.get('VALUE', '')}\t{curve_info.get('API_CODE', '')}"
                        metadata_rows.append(row)
                else:
                    for line in sections['curve']:
                        if line.strip() and not line.startswith('#'):
                            metadata_rows.append(line)
                metadata_rows.append("")  # LÃ­nea en blanco
            
            # Agregar informaciÃ³n del pozo
            if 'well' in sections and sections['well']:
                metadata_rows.append("=== WELL INFORMATION SECTION ===")
                metadata_rows.append("MNEMONIC\tUNIT\tVALUE\tDESCRIPTION")
                metadata_rows.append("--------\t----\t-----\t-----------")
                
                if isinstance(sections['well'][0], dict):
                    for well_info in sections['well']:
                        row = f"{well_info.get('MNEMONIC', '')}\t{well_info.get('UNIT', '')}\t{well_info.get('VALUE', '')}\t{well_info.get('DESCRIPTION', '')}"
                        metadata_rows.append(row)
                else:
                    for line in sections['well']:
                        if line.strip() and not line.startswith('#'):
                            metadata_rows.append(line)
                metadata_rows.append("")  # LÃ­nea en blanco
            
            # Agregar informaciÃ³n de parÃ¡metros
            if 'parameter' in sections and sections['parameter']:
                metadata_rows.append("=== PARAMETER INFORMATION SECTION ===")
                metadata_rows.append("MNEMONIC\tUNIT\tVALUE\tDESCRIPTION")
                metadata_rows.append("--------\t----\t-----\t-----------")
                
                if isinstance(sections['parameter'][0], dict):
                    for param_info in sections['parameter']:
                        row = f"{param_info.get('MNEMONIC', '')}\t{param_info.get('UNIT', '')}\t{param_info.get('VALUE', '')}\t{param_info.get('DESCRIPTION', '')}"
                        metadata_rows.append(row)
                else:
                    for line in sections['parameter']:
                        if line.strip() and not line.startswith('#'):
                            metadata_rows.append(line)
                metadata_rows.append("")  # LÃ­nea en blanco
            
            # Crear DataFrame de metadatos
            if metadata_rows:
                metadata_df = pd.DataFrame(metadata_rows, columns=['METADATA'])
                metadata_df.to_excel(writer, sheet_name='METADATOS', index=False)
            
            # ===== HOJA 3: RESUMEN ESTADÃSTICO =====
            if not df.empty:
                stats_df = df.describe(include='all')
                stats_df.to_excel(writer, sheet_name='ESTADISTICAS')
        
        output.seek(0)
        
        st.download_button(
            label="ðŸ“¥ Descargar como Excel (Completo)",
            data=output,
            file_name=nombre_archivo,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        return True
    except Exception as e:
        st.error(f"Error al crear Excel: {e}")
        return False

def descargar_como_excel(df, nombre_archivo="datos_convertidos.xlsx"):
    """Crea archivo Excel descargable simple"""
    try:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Datos_Pozo')
        output.seek(0)
        
        st.download_button(
            label="ðŸ“¥ Descargar como Excel",
            data=output,
            file_name=nombre_archivo,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        return True
    except Exception as e:
        st.error(f"Error al crear Excel: {e}")
        return False

def detect_file_type(archivo):
    """Detecta tipo de archivo de manera mÃ¡s robusta"""
    try:
        # Guardar posiciÃ³n actual
        current_pos = archivo.tell()
        
        # Leer primeros bytes para verificar signature
        primeros_bytes = archivo.read(10)
        archivo.seek(current_pos)  # Volver al inicio
        
        # Verificar si es Excel por signature
        if primeros_bytes.startswith(b'PK'):  # ZIP signature (Excel es ZIP)
            return 'EXCEL'
        
        # Leer contenido para verificar LAS
        contenido = archivo.read(2000).decode('utf-8', errors='ignore')
        archivo.seek(current_pos)  # Volver al inicio
        
        # Verificar patrones LAS
        las_patterns = ['~VERSION', '~WELL', '~CURVE', '~A']
        if any(pattern in contenido for pattern in las_patterns):
            return 'LAS'
        
        # Verificar por extensiÃ³n como fallback
        nombre_archivo = archivo.name.lower()
        if nombre_archivo.endswith('.las'):
            return 'LAS'
        elif nombre_archivo.endswith(('.xlsx', '.xls')):
            return 'EXCEL'
        else:
            return 'UNKNOWN'
            
    except Exception as e:
        # Fallback por extensiÃ³n
        try:
            nombre_archivo = archivo.name.lower()
            if nombre_archivo.endswith('.las'):
                return 'LAS'
            elif nombre_archivo.endswith(('.xlsx', '.xls')):
                return 'EXCEL'
            else:
                return 'UNKNOWN'
        except:
            return 'UNKNOWN'
# ================================
# ðŸ”„ FUNCIONES DE MULTICARGA DE ARCHIVOS
# ================================
def procesar_multiple_archivos(archivos):
    """Procesa mÃºltiples archivos y retorna DataFrames combinados"""
    dataframes = []
    info_archivos = []
    
    for i, archivo in enumerate(archivos):
        with st.spinner(f"Procesando archivo {i+1}/{len(archivos)}: {archivo.name}..."):
            file_type = detect_file_type(archivo)
            
            if file_type == 'LAS':
                # Para LAS, procesar y convertir
                if LASIO_AVAILABLE:
                    df_temp, sections = procesar_las_con_lasio(archivo)
                else:
                    df_temp, sections = procesar_las_manual(archivo)
                
                if df_temp is not None:
                    # Agregar columna de identificador
                    df_temp['ARCHIVO_ORIGEN'] = archivo.name
                    dataframes.append(df_temp)
                    info_archivos.append({
                        'nombre': archivo.name,
                        'tipo': 'LAS',
                        'filas': len(df_temp),
                        'columnas': len(df_temp.columns)
                    })
                    
            elif file_type == 'EXCEL':
                # Para Excel, cargar directamente
                try:
                    archivo.seek(0)
                    if archivo.name.endswith('.xlsx'):
                        df_temp = pd.read_excel(archivo, engine='openpyxl')
                    elif archivo.name.endswith('.xls'):
                        df_temp = pd.read_excel(archivo, engine='xlrd')
                    else:
                        df_temp = pd.read_excel(archivo, engine='openpyxl')
                    
                    # Agregar columna de identificador
                    df_temp['ARCHIVO_ORIGEN'] = archivo.name
                    dataframes.append(df_temp)
                    info_archivos.append({
                        'nombre': archivo.name,
                        'tipo': 'Excel',
                        'filas': len(df_temp),
                        'columnas': len(df_temp.columns)
                    })
                    
                except Exception as e:
                    st.error(f"âŒ Error cargando {archivo.name}: {str(e)}")
    
    if not dataframes:
        st.error("âŒ No se pudieron procesar los archivos")
        return None, None
    
    # Combinar DataFrames
    try:
        # Encontrar columnas comunes para el merge
        common_columns = set(dataframes[0].columns)
        for df in dataframes[1:]:
            common_columns = common_columns.intersection(set(df.columns))
        
        # Convertir a lista y ordenar
        common_columns = sorted(list(common_columns))
        
        if not common_columns:
            st.error("âŒ No hay columnas comunes entre los archivos")
            return None, None
        
        # Combinar DataFrames
        df_combinado = pd.concat(dataframes, axis=0, ignore_index=True)
        
        st.success(f"âœ… {len(dataframes)} archivos combinados: {len(df_combinado)} filas totales")
        return df_combinado, info_archivos
        
    except Exception as e:
        st.error(f"âŒ Error combinando archivos: {str(e)}")
        return None, None

# ================================
# ðŸŽ¯ INTERFAZ MEJORADA PARA LAS CON LASIO
# ================================
def interfaz_las_mejorada(archivo):
    """Interfaz mejorada para archivos LAS con informaciÃ³n de curvas usando lasio"""
    
    st.success(f"ðŸ“ Archivo LAS detectado: {archivo.name}")
    
    # Mostrar opciones de procesamiento
    st.subheader("ðŸ”„ Opciones de Procesamiento")
    
    if LASIO_AVAILABLE:
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            ### ðŸš€ **Procesamiento Profesional con Lasio** (Recomendado)
            **Incluye:**
            â€¢ InformaciÃ³n completa de curvas con metadatos
            â€¢ Nombres reales de columnas del archivo LAS
            â€¢ Unidades y descripciones de curvas
            â€¢ Metadatos estructurados del pozo
            â€¢ Manejo robusto de formatos LAS
            """)
            
            if st.button("ðŸ”„ Procesar con Lasio", type="primary", use_container_width=True, key="procesar_lasio"):
                with st.spinner("Procesando archivo LAS con Lasio..."):
                    # Resetear archivo a posiciÃ³n inicial
                    archivo.seek(0)
                    df, sections = procesar_las_con_lasio(archivo)
                    
                    if df is not None and sections is not None:
                        mostrar_resultados_las(df, sections, archivo.name, "Lasio")
    
    else:
        st.markdown("""
        ### âš ï¸ **Procesamiento BÃ¡sico** (Lasio no disponible)
        **Instala Lasio para mejor experiencia:**
        ```bash
        pip install lasio
        ```
        """)
    
    # OpciÃ³n de procesamiento manual (siempre disponible)
    col_manual1, col_manual2 = st.columns(2) if LASIO_AVAILABLE else st.columns(1)
    
    with col_manual1:
        st.markdown("""
        ### ðŸ”§ **Procesamiento Manual**
        **CaracterÃ­sticas:**
        â€¢ ExtracciÃ³n bÃ¡sica de datos
        â€¢ Nombres de columnas predefinidos
        â€¢ Formato simple pero funcional
        â€¢ Siempre disponible
        """)
        
        if st.button("ðŸ”§ Procesamiento Manual", use_container_width=True, 
                    type="primary" if not LASIO_AVAILABLE else "secondary",
                    key="procesar_manual"):
            with st.spinner("Procesando archivo LAS (manual)..."):
                # Resetear archivo a posiciÃ³n inicial
                archivo.seek(0)
                df, sections = procesar_las_manual(archivo)
                
                if df is not None and sections is not None:
                    mostrar_resultados_las(df, sections, archivo.name, "Manual")

def mostrar_resultados_las(df, sections, nombre_archivo, metodo):
    """Muestra los resultados del procesamiento LAS"""
    st.balloons()
    st.success(f"âœ… Â¡Archivo procesado exitosamente con {metodo}!")
    
    # Mostrar informaciÃ³n de curvas
    st.subheader("ðŸ“‹ InformaciÃ³n de Curvas")
    if 'curve' in sections and sections['curve']:
        with st.expander("ðŸ” Ver detalles de curvas", expanded=True):
            # Verificar el tipo de datos en sections
            if isinstance(sections['curve'][0], dict):
                # Datos de lasio (diccionarios)
                curve_df = pd.DataFrame(sections['curve'])
                st.dataframe(curve_df, use_container_width=True)
            else:
                # Datos manuales (strings)
                for line in sections['curve']:
                    if line.strip() and not line.startswith('#'):
                        st.code(line)
    
    # Mostrar informaciÃ³n del pozo
    if 'well' in sections and sections['well']:
        with st.expander("ðŸ­ InformaciÃ³n del Pozo", expanded=False):
            if isinstance(sections['well'][0], dict):
                well_df = pd.DataFrame(sections['well'])
                st.dataframe(well_df, use_container_width=True)
            else:
                for line in sections['well']:
                    if line.strip() and not line.startswith('#'):
                        st.code(line)
    
    # Mostrar datos
    st.subheader("ðŸ“Š Datos Procesados")
    st.dataframe(df.head(10))
    st.info(f"**Estructura:** {len(df)} filas Ã— {len(df.columns)} columnas")
    st.info(f"**Columnas:** {list(df.columns)}")
    
    # EstadÃ­sticas bÃ¡sicas
    with st.expander("ðŸ“ˆ EstadÃ­sticas BÃ¡sicas", expanded=False):
        st.dataframe(df.describe(), use_container_width=True)
    
    # Descargar
    st.subheader("ðŸ’¾ Descargar Resultado")
    nombre_descarga = nombre_archivo.replace('.las', f'_{metodo.upper()}.xlsx').replace('.LAS', f'_{metodo.upper()}.xlsx')
    
    if descargar_las_como_excel(df, sections, nombre_descarga):
        st.success(f"""
        **ðŸ“ Siguientes pasos ({metodo}):**
        1. **Descarga** el archivo Excel completo
        2. **Vuelve** a cargar el Excel descargado  
        3. **Usa** los mÃ³dulos de anÃ¡lisis
        4. **Consulta** la hoja METADATOS para informaciÃ³n de curvas
        """)
# ================================
# ðŸŽ¯ FUNCIÃ“N DE CARGA PRINCIPAL MEJORADA CON LASIO Y MULTICARGA
# ================================
def cargar_datos(archivo=None, archivos=None):
    """FunciÃ³n principal para cargar datos - MEJORADA CON LASIO Y MULTICARGA"""
    
    # Manejar multicarga
    if archivos and len(archivos) > 1:
        st.info(f"ðŸ“ {len(archivos)} archivos seleccionados para procesamiento mÃºltiple")
        
        # Mostrar informaciÃ³n de archivos
        with st.expander("ðŸ“‹ Archivos seleccionados", expanded=True):
            for i, archivo in enumerate(archivos):
                file_type = detect_file_type(archivo)
                st.write(f"{i+1}. **{archivo.name}** - Tipo: {file_type}")
        
        # Procesar mÃºltiples archivos
        if st.button("ðŸ”„ Procesar MÃºltiples Archivos", type="primary", use_container_width=True):
            df_combinado, info_archivos = procesar_multiple_archivos(archivos)
            
            if df_combinado is not None:
                st.session_state.df_actual = df_combinado
                st.session_state.multicarga_info = info_archivos
                
                # Mostrar resumen de multicarga
                st.subheader("ðŸ“Š Resumen de Multicarga")
                info_df = pd.DataFrame(info_archivos)
                st.dataframe(info_df, use_container_width=True)
                
                # Mostrar datos combinados
                st.subheader("ðŸ“ˆ Datos Combinados")
                st.dataframe(df_combinado.head(15))
                st.info(f"**Estructura total:** {len(df_combinado)} filas Ã— {len(df_combinado.columns)} columnas")
                
                # OpciÃ³n para descargar datos combinados
                st.subheader("ðŸ’¾ Descargar Datos Combinados")
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df_combinado.to_excel(writer, sheet_name='DATOS_COMBINADOS', index=False)
                    info_df.to_excel(writer, sheet_name='INFO_ARCHIVOS', index=False)
                
                output.seek(0)
                st.download_button(
                    label="ðŸ“¥ Descargar Datos Combinados",
                    data=output,
                    file_name=f"datos_combinados_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
                
                return df_combinado
        
        return None
    
    # Manejar archivo Ãºnico (comportamiento original)
    elif archivo:
        try:
            file_type = detect_file_type(archivo)
            
            st.info(f"ðŸ“Š Tipo de archivo detectado: {file_type}")
            
            if file_type == 'LAS':
                # Para archivos LAS, mostrar la interfaz de procesamiento mejorada
                interfaz_las_mejorada(archivo)
                return None
                
            elif file_type == 'EXCEL':
                # Para Excel, carga normal con manejo de errores
                try:
                    # Resetear archivo a posiciÃ³n inicial
                    archivo.seek(0)
                    
                    if archivo.name.endswith('.xlsx'):
                        df = pd.read_excel(archivo, engine='openpyxl')
                    elif archivo.name.endswith('.xls'):
                        df = pd.read_excel(archivo, engine='xlrd')
                    else:
                        df = pd.read_excel(archivo, engine='openpyxl')
                    
                    st.success(f"âœ… Excel cargado: {len(df)} filas, {len(df.columns)} columnas")
                    return df
                    
                except Exception as excel_error:
                    st.error(f"âŒ Error cargando Excel: {str(excel_error)}")
                    st.info("ðŸ’¡ Â¿EstÃ¡s seguro de que es un archivo Excel vÃ¡lido?")
                    return None
                    
            else:
                st.error("âŒ Tipo de archivo no reconocido")
                st.info("""
                **Formatos soportados:**
                â€¢ Excel (.xlsx, .xls)
                â€¢ LAS (.las)
                
                **Por favor verifica:**
                â€¢ Que el archivo no estÃ© corrupto
                â€¢ Que sea uno de los formatos soportados
                """)
                return None
                
        except Exception as e:
            st.error(f"âŒ Error general al cargar archivo: {str(e)}")
            return None
    
    return None

# ================================
# ðŸŽ›ï¸ FUNCIÃ“N PARA MOSTRAR CARGADOR CON MULTICARGA (CORREGIDA)
# ================================
def mostrar_cargador_datos(modulo_nombre, instrucciones_especificas="", permitir_multicarga=True):
    st.header(f"{modulo_nombre}")
    st.subheader("ðŸ“‚ Carga de Datos")
    
    # Inicializar variables para evitar UnboundLocalError
    archivos = None
    archivo = None
    
    # ConfiguraciÃ³n de multicarga
    if permitir_multicarga:
        col_multicarga1, col_multicarga2 = st.columns(2)
        
        with col_multicarga1:
            modo_carga = st.radio(
                "Modo de carga:",
                ["Archivo Ãºnico", "MÃºltiples archivos"],
                horizontal=True
            )
        
        with col_multicarga2:
            if modo_carga == "MÃºltiples archivos":
                st.info("ðŸ”— Los archivos se combinarÃ¡n automÃ¡ticamente")
    
    # Cargador de archivos
    if permitir_multicarga and modo_carga == "MÃºltiples archivos":
        archivos = st.file_uploader(
            f"Sube tus archivos Excel o LAS", 
            type=["xlsx", "xls", "las"],
            key=f"cargador_multiple_{modulo_nombre}",
            accept_multiple_files=True
        )
        
        if archivos:
            resultado = cargar_datos(archivos=archivos)
            if resultado is not None:
                st.session_state.df_actual = resultado
                return resultado
            return None
            
    else:
        archivo = st.file_uploader(
            f"Sube tu archivo Excel o LAS", 
            type=["xlsx", "xls", "las"],
            key=f"cargador_{modulo_nombre}"
        )
        
        if archivo is not None:
            file_type = detect_file_type(archivo)
            
            if file_type == 'LAS':
                st.info("""
                **ðŸ”„ Procesamiento LAS MEJORADO:**
                â€¢ ExtracciÃ³n completa con informaciÃ³n de curvas
                â€¢ Nombres reales de columnas
                â€¢ Metadatos del pozo incluidos
                â€¢ Estructura profesional en Excel
                """)
            
            resultado = cargar_datos(archivo=archivo)
            
            # Solo procesar si es Excel y tenemos un DataFrame
            if file_type != 'LAS' and resultado is not None:
                st.session_state.df_actual = resultado
                st.success(f"âœ… Datos cargados: {len(resultado)} filas, {len(resultado.columns)} columnas")
                
                with st.expander("ðŸ“Š Vista previa", expanded=True):
                    st.dataframe(resultado.head(10))
                    st.info(f"**Columnas:** {list(resultado.columns)}")
                    
                return resultado
            
            # Para LAS, no retornamos DataFrame porque el usuario debe descargar el Excel primero
            return None
    
    # Mostrar instrucciones si no hay archivos (CORREGIDO)
    # Ahora ambas variables estÃ¡n siempre definidas
    if not archivos and not archivo:
        st.info(f"""
        **ðŸ“ Instrucciones para {modulo_nombre}:**
        
        {instrucciones_especificas}
        
        **ðŸ“‹ Formatos:**
        â€¢ **Excel (.xlsx, .xls)** - Carga directa
        â€¢ **LAS (.las)** - ConversiÃ³n automÃ¡tica a Excel
        
        **ðŸ’¡ Para archivos LAS (MEJORADO):**
        Se convertirÃ¡n automÃ¡ticamente a Excel con informaciÃ³n completa de curvas.
        
        **ðŸ”„ Multicarga disponible:** Puedes cargar mÃºltiples archivos y se combinarÃ¡n automÃ¡ticamente.
        """)
        return None
    
    return None
# ================================
# ðŸ“Š MÃ“DULO 1: VISUALIZACIÃ“N BÃSICA (ACTUALIZADO CON FUNCIONALIDADES AVANZADAS)
# ================================
def modulo_visualizacion_basica():
    
    # Instrucciones especÃ­ficas para visualizaciÃ³n
    instrucciones_visualizacion = """
    1. **Selecciona la columna de profundidad** que serÃ¡ tu eje Y
    2. **Elige las curvas** que quieres visualizar en los ejes X
    3. **Personaliza** colores, estilos y configuraciÃ³n de ejes
    4. **Ajusta el rango** de profundidad si es necesario
    5. **Agrega marcadores estratigrÃ¡ficos** para identificar formaciones
    6. **Usa plantillas predefinidas** para diferentes ambientes geolÃ³gicos
    7. **Compara mÃºltiples pozos** (modo multicarga)
    8. **Aplica normalizaciones** para mejorar la visualizaciÃ³n
    9. **Analiza correlaciones** entre curvas seleccionadas
    """
    
    df = mostrar_cargador_datos(
        "VisualizaciÃ³n BÃ¡sica", 
        instrucciones_visualizacion,
        permitir_multicarga=True
    )
    
    if df is None:
        return

    # Verificar si es multicarga
    es_multicarga = 'ARCHIVO_ORIGEN' in df.columns if df is not None else False
    
    if es_multicarga:
        st.info("ðŸŽ¯ **Modo Multicarga Activado** - Visualizando datos combinados de mÃºltiples archivos")
        
        # Selector de archivo para filtrar
        archivos_unicos = df['ARCHIVO_ORIGEN'].unique()
        archivo_seleccionado = st.selectbox(
            "Filtrar por archivo (opcional):",
            options=["Todos los archivos"] + list(archivos_unicos)
        )
        
        # Aplicar filtro si se selecciona un archivo especÃ­fico
        if archivo_seleccionado != "Todos los archivos":
            df = df[df['ARCHIVO_ORIGEN'] == archivo_seleccionado]
            st.info(f"ðŸ“ Mostrando datos de: {archivo_seleccionado}")

    # ================================
    # ðŸŽ¨ PLANTILLAS PREDEFINIDAS
    # ================================
    st.subheader("ðŸŽ¨ Plantillas de VisualizaciÃ³n")
    
    col_temp1, col_temp2, col_temp3 = st.columns(3)
    
    with col_temp1:
        plantilla_seleccionada = st.selectbox(
            "Plantilla geolÃ³gica:",
            options=["Personalizada", "ClÃ¡stico (Triple Combo)", "Carbonato", "Lutitas (Unconventional)", "Aguas Profundas", "BÃ¡sica"],
            help="Selecciona una plantilla predefinida para configuraciÃ³n automÃ¡tica"
        )
    
    with col_temp2:
        # Aplicar plantilla
        if st.button("ðŸ”„ Aplicar Plantilla", use_container_width=True):
            st.session_state.aplicar_plantilla = True
    
    with col_temp3:
        # Nombre personalizado para la grÃ¡fica
        nombre_grafica = st.text_input(
            "Nombre de la grÃ¡fica:",
            value=f"Registros_{datetime.now().strftime('%H%M')}",
            help="Asigna un nombre personalizado a tu grÃ¡fica"
        )

    # ConfiguraciÃ³n de plantillas
    configuraciones_plantillas = {
        "ClÃ¡stico (Triple Combo)": {
            "curvas_sugeridas": ['GR', 'RT', 'RHOB', 'NPHI', 'CALI'],
            "colores": ['green', 'red', 'blue', 'purple', 'orange'],
            "escalas": {'GR': (0, 150), 'RT': (0.2, 2000), 'RHOB': (1.8, 2.8), 'NPHI': (0.45, -0.15)},
            "eje_secundario": ['RT']
        },
        "Carbonato": {
            "curvas_sugeridas": ['GR', 'RT', 'RHOB', 'NPHI', 'DT', 'PEF'],
            "colores": ['darkgreen', 'darkred', 'navy', 'darkviolet', 'brown', 'gray'],
            "escalas": {'GR': (0, 100), 'RT': (1, 1000), 'RHOB': (2.3, 2.8), 'NPHI': (0.3, -0.05), 'DT': (40, 140)},
            "eje_secundario": ['RT', 'DT']
        },
        "Lutitas (Unconventional)": {
            "curvas_sugeridas": ['GR', 'RT', 'RHOB', 'NPHI', 'CALI', 'SP'],
            "colores": ['brown', 'red', 'blue', 'purple', 'orange', 'teal'],
            "escalas": {'GR': (50, 200), 'RT': (1, 100), 'RHOB': (2.4, 2.9), 'NPHI': (0.25, 0.05)},
            "eje_secundario": ['RT']
        },
        "Aguas Profundas": {
            "curvas_sugeridas": ['GR', 'RD', 'RS', 'RHOB', 'NPHI', 'CALI'],
            "colores": ['green', 'red', 'darkred', 'blue', 'purple', 'orange'],
            "escalas": {'GR': (0, 150), 'RD': (0.2, 200), 'RS': (0.2, 200), 'RHOB': (1.8, 2.8), 'NPHI': (0.45, -0.15)},
            "eje_secundario": ['RD', 'RS']
        },
        "BÃ¡sica": {
            "curvas_sugeridas": ['GR', 'RT', 'RHOB'],
            "colores": ['green', 'red', 'blue'],
            "escalas": {},
            "eje_secundario": ['RT']
        }
    }

    # --- SelecciÃ³n de columna de profundidad (Eje Y) ---
    st.subheader("ðŸŽ¯ ConfiguraciÃ³n de Ejes")
    
    # Filtrar columnas numÃ©ricas para profundidad
    columnas_numericas = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
    
    if not columnas_numericas:
        st.error("âŒ No se encontraron columnas numÃ©ricas en el archivo.")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Selecciona la columna para el eje Y (Profundidad):**")
        columna_profundidad = st.selectbox(
            "Columna de profundidad:",
            options=columnas_numericas,
            index=0,
            help="Selecciona la columna que representa la profundidad o tiempo"
        )
        
        # Guardar en sesiÃ³n
        st.session_state.columna_profundidad = columna_profundidad
        
        # Mostrar informaciÃ³n de la columna seleccionada
        st.info(f"""
        **InformaciÃ³n de {columna_profundidad}:**
        - MÃ­nimo: {df[columna_profundidad].min():.2f}
        - MÃ¡ximo: {df[columna_profundidad].max():.2f}
        - Valores Ãºnicos: {df[columna_profundidad].nunique()}
        """)
    
    with col2:
        st.markdown("**Opciones de profundidad:**")
        invertir_eje_y = st.checkbox("Invertir eje Y", value=True, 
                                   help="Invertir el eje Y para que la profundidad aumente hacia abajo")
        
        # Renombrar columna temporalmente para consistencia
        df_temp = df.rename(columns={columna_profundidad: "PROFUNDIDAD"})

    # ================================
    # ðŸ”§ NORMALIZACIÃ“N DE CURVAS
    # ================================
    st.subheader("ðŸ”§ NormalizaciÃ³n de Curvas")
    
    col_norm1, col_norm2, col_norm3 = st.columns(3)
    
    with col_norm1:
        aplicar_normalizacion = st.checkbox(
            "Aplicar normalizaciÃ³n", 
            value=False,
            help="Normalizar curvas para mejor comparaciÃ³n visual"
        )
    
    with col_norm2:
        if aplicar_normalizacion:
            metodo_normalizacion = st.selectbox(
                "MÃ©todo de normalizaciÃ³n:",
                options=["Z-score", "Min-Max", "Robust (IQR)", "Percentil (5-95%)"],
                help="Selecciona el mÃ©todo de normalizaciÃ³n"
            )
    
    with col_norm3:
        if aplicar_normalizacion:
            normalizar_por_archivo = st.checkbox(
                "Normalizar por archivo individual", 
                value=True,
                disabled=not es_multicarga,
                help="Aplicar normalizaciÃ³n separada para cada archivo en multicarga"
            )

    # --- Filtrar columnas para curvas (Eje X) ---
    columnas_para_usar = [col for col in columnas_numericas if col != columna_profundidad]

    if len(columnas_para_usar) == 0:
        st.error("No se encontraron columnas numÃ©ricas vÃ¡lidas para graficar (ademÃ¡s de la columna de profundidad).")
        return

    # ================================
    # ðŸ” SELECCIÃ“N DE RANGO DE PROFUNDIDAD
    # ================================
    st.subheader("ðŸ” Selecciona el rango de profundidad para graficar")

    # Asegurar que la columna de profundidad sea numÃ©rica
    df_temp["PROFUNDIDAD"] = pd.to_numeric(df_temp["PROFUNDIDAD"], errors="coerce")
    df_temp = df_temp.dropna(subset=["PROFUNDIDAD"])

    prof_min_global = float(df_temp["PROFUNDIDAD"].min())
    prof_max_global = float(df_temp["PROFUNDIDAD"].max())

    # Redondear para mejor visualizaciÃ³n
    prof_min_red = round(prof_min_global, 1)
    prof_max_red = round(prof_max_global, 1)

    profundidad_min, profundidad_max = st.slider(
        f"Rango de {columna_profundidad}",
        min_value=prof_min_red,
        max_value=prof_max_red,
        value=(prof_min_red, prof_max_red),
        step=0.1,
        format="%.1f"
    )

    # Filtrar el DataFrame al rango seleccionado
    df_filtrado = df_temp[(df_temp["PROFUNDIDAD"] >= profundidad_min) & (df_temp["PROFUNDIDAD"] <= profundidad_max)].copy()

    if df_filtrado.empty:
        st.error("âŒ No hay datos en el rango de profundidad seleccionado.")
        return

    # Reemplazar `df_temp` por `df_filtrado` para el resto del flujo
    df_temp = df_filtrado

    # ================================
    # ðŸŽ¯ CONFIGURACIÃ“N DE MARCADORES ESTRATIGRÃFICOS
    # ================================
    st.subheader("ðŸŽ¯ Marcadores EstratigrÃ¡ficos (Topes)")
    
    col_marc1, col_marc2 = st.columns([2, 1])
    
    with col_marc1:
        agregar_marcadores = st.checkbox(
            "Agregar marcadores estratigrÃ¡ficos", 
            value=False,
            help="Agrega lÃ­neas horizontales para identificar formaciones geolÃ³gicas"
        )
    
    with col_marc2:
        if agregar_marcadores:
            num_marcadores = st.number_input(
                "NÃºmero de marcadores:", 
                min_value=1, 
                max_value=20, 
                value=3,
                help="Cantidad de topes estratigrÃ¡ficos a agregar"
            )

    # ConfiguraciÃ³n de marcadores
    marcadores = []
    if agregar_marcadores:
        st.markdown("#### ðŸ“ ConfiguraciÃ³n de Marcadores")
        
        # Colores para marcadores
        colores_marcadores = [
            'red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray',
            'olive', 'cyan', 'magenta', 'darkred', 'darkblue', 'darkgreen',
            'darkviolet', 'gold', 'lime', 'teal', 'navy', 'maroon'
        ]
        
        # Estilos de lÃ­nea para marcadores
        estilos_marcadores = ['-', '--', '-.', ':']
        
        for i in range(int(num_marcadores)):
            st.markdown(f"---")
            st.markdown(f"#### ðŸ“Œ Marcador {i+1}")
            
            col_m1, col_m2, col_m3, col_m4 = st.columns(4)
            
            with col_m1:
                profundidad_marcador = st.number_input(
                    f"Profundidad del tope {i+1}",
                    min_value=float(profundidad_min),
                    max_value=float(profundidad_max),
                    value=float(profundidad_min + (i * (profundidad_max - profundidad_min) / num_marcadores)),
                    step=0.1,
                    key=f"prof_marc_{i}"
                )
            
            with col_m2:
                nombre_marcador = st.text_input(
                    f"Nombre formaciÃ³n {i+1}",
                    value=f"FormaciÃ³n {i+1}",
                    key=f"nombre_marc_{i}"
                )
            
            with col_m3:
                color_marcador = st.selectbox(
                    f"Color {i+1}",
                    options=colores_marcadores,
                    index=i % len(colores_marcadores),
                    key=f"color_marc_{i}"
                )
            
            with col_m4:
                estilo_marcador = st.selectbox(
                    f"Estilo lÃ­nea {i+1}",
                    options=estilos_marcadores,
                    format_func=lambda x: {"-": "SÃ³lida", "--": "Punteada", "-.": "Trazo-punto", ":": "Puntos"}[x],
                    index=i % len(estilos_marcadores),
                    key=f"estilo_marc_{i}"
                )
            
            # Agregar anotaciÃ³n opcional
            anotacion_marcador = st.text_area(
                f"AnotaciÃ³n para {nombre_marcador} (opcional)",
                value="",
                placeholder="DescripciÃ³n de la formaciÃ³n, caracterÃ­sticas, etc.",
                key=f"anot_marc_{i}"
            )
            
            marcadores.append({
                "profundidad": profundidad_marcador,
                "nombre": nombre_marcador,
                "color": color_marcador,
                "estilo": estilo_marcador,
                "anotacion": anotacion_marcador
            })

    # ================================
    # ðŸ“ˆ CONFIGURACIÃ“N DE CURVAS (ACTUALIZADO CON PLANTILLAS)
    # ================================
    st.subheader("ðŸ“ˆ ConfiguraciÃ³n de curvas")

    # NÃºmero de curvas a mostrar
    num_curvas = st.number_input("NÃºmero de curvas a mostrar", min_value=1, max_value=10, value=2)

    # Aplicar configuraciÃ³n de plantilla si se seleccionÃ³
    if plantilla_seleccionada != "Personalizada" and st.session_state.get('aplicar_plantilla', False):
        plantilla = configuraciones_plantillas[plantilla_seleccionada]
        curvas_sugeridas = [c for c in plantilla['curvas_sugeridas'] if c in columnas_para_usar]
        num_curvas = min(len(curvas_sugeridas), num_curvas)
        st.session_state.aplicar_plantilla = False
        st.success(f"âœ… Plantilla {plantilla_seleccionada} aplicada")

    # Listas y opciones
    colores_validos = [
        'black', 'red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink',
        'gray', 'olive', 'cyan', 'magenta', 'darkred', 'darkblue', 'darkgreen',
        'darkviolet', 'gold', 'lime', 'teal', 'navy', 'maroon'
    ]

    estilos_sugeridos = ['-', '--', '-.', ':']

    orden_z = {"Fondo": 1, "Medio": 3, "Delante": 5}

    curvas = []

    for i in range(int(num_curvas)):
        st.markdown(f"---")
        st.markdown(f"#### ðŸ“Œ Curva {i+1}")

        col1, col2, col3, col4, col5, col6, col7, col8, col9 = st.columns(9)

        with col1:
            # Sugerir curvas basadas en plantilla
            if plantilla_seleccionada != "Personalizada" and i < len(curvas_sugeridas):
                default_col = curvas_sugeridas[i] if curvas_sugeridas[i] in columnas_para_usar else columnas_para_usar[min(i, len(columnas_para_usar)-1)]
            else:
                default_col = columnas_para_usar[min(i, len(columnas_para_usar)-1)]
            
            col_seleccionada = st.selectbox(
                f"Selecciona columna (Curva {i+1})",
                options=columnas_para_usar,
                index=columnas_para_usar.index(default_col) if default_col in columnas_para_usar else min(i, len(columnas_para_usar)-1),
                key=f"col_{i}"
            )

        usos_previos = sum(1 for c in curvas if c["columna"] == col_seleccionada)
        etiqueta_default = f"{col_seleccionada} ({usos_previos + 1})" if usos_previos > 0 else col_seleccionada

        with col2:
            # Color basado en plantilla
            if plantilla_seleccionada != "Personalizada" and i < len(plantilla['colores']):
                color_default = plantilla['colores'][i]
                color_index = colores_validos.index(color_default) if color_default in colores_validos else i % len(colores_validos)
            else:
                color_index = i % len(colores_validos)
            
            color = st.selectbox(f"Color (Curva {i+1})", options=colores_validos, index=color_index, key=f"color_{i}")
        
        with col3:
            estilo = st.selectbox(
                f"Estilo de lÃ­nea (Curva {i+1})",
                options=estilos_sugeridos,
                format_func=lambda x: {"-": "SÃ³lida", "--": "Punteada", "-.": "Trazo-punto", ":": "Puntos"}[x],
                index=i % len(estilos_sugeridos),
                key=f"estilo_{i}"
            )
        
        with col4:
            etiqueta = st.text_input(f"Etiqueta en leyenda", value=etiqueta_default, key=f"etiqueta_{i}")
        
        with col5:
            # Eje secundario basado en plantilla
            if plantilla_seleccionada != "Personalizada" and col_seleccionada in plantilla['eje_secundario']:
                eje_secundario_default = True
            else:
                eje_secundario_default = False
            
            eje_secundario = st.checkbox(f"Eje X superior", value=eje_secundario_default, key=f"eje_sec_{i}")
        
        with col6:
            suavizar = st.checkbox(f"Suavizar", key=f"suavizar_{i}")
        
        with col7:
            ventana = st.number_input(f"Ventana (puntos)", min_value=2, max_value=50, value=5, step=1, key=f"ventana_{i}", disabled=not suavizar)
        
        with col8:
            invertir_x = st.checkbox(f"Invertir eje X", key=f"invertir_x_{i}")
        
        with col9:
            capa = st.selectbox(f"PosiciÃ³n", options=list(orden_z.keys()), index=1, key=f"capa_{i}")

        nombre_base_limpio = re.sub(r'[^a-zA-Z0-9_]', '_', col_seleccionada)
        nombre_interno_limpio = f"curva_{nombre_base_limpio}_{i+1}"

        curvas.append({
            "columna": col_seleccionada,
            "color": color,
            "estilo": estilo,
            "etiqueta": etiqueta,
            "eje_secundario": eje_secundario,
            "suavizar": suavizar,
            "ventana": ventana if suavizar else 1,
            "invertir_x": invertir_x,
            "zorder": orden_z[capa],
            "nombre_interno": nombre_interno_limpio
        })
    
    # --- Alinear todas las curvas en profundidad comÃºn ---
    st.subheader("âš™ï¸ Procesando datos...")

    # Aplicar normalizaciÃ³n si estÃ¡ activada
    if aplicar_normalizacion:
        df_procesado = df_temp.copy()
        
        if es_multicarga and normalizar_por_archivo:
            # Normalizar por archivo individual
            df_procesado_normalizado = pd.DataFrame()
            for archivo in df_procesado['ARCHIVO_ORIGEN'].unique():
                df_archivo = df_procesado[df_procesado['ARCHIVO_ORIGEN'] == archivo].copy()
                
                for curva in curvas:
                    col_data = df_archivo[curva['columna']].dropna()
                    if len(col_data) > 0:
                        if metodo_normalizacion == "Z-score":
                            df_archivo[f"{curva['columna']}_NORM"] = (col_data - col_data.mean()) / col_data.std()
                        elif metodo_normalizacion == "Min-Max":
                            df_archivo[f"{curva['columna']}_NORM"] = (col_data - col_data.min()) / (col_data.max() - col_data.min())
                        elif metodo_normalizacion == "Robust (IQR)":
                            Q1 = col_data.quantile(0.25)
                            Q3 = col_data.quantile(0.75)
                            IQR = Q3 - Q1
                            df_archivo[f"{curva['columna']}_NORM"] = (col_data - col_data.median()) / IQR
                        elif metodo_normalizacion == "Percentil (5-95%)":
                            p5 = col_data.quantile(0.05)
                            p95 = col_data.quantile(0.95)
                            df_archivo[f"{curva['columna']}_NORM"] = (col_data - p5) / (p95 - p5)
                
                df_procesado_normalizado = pd.concat([df_procesado_normalizado, df_archivo])
            
            df_procesado = df_procesado_normalizado
            # Actualizar nombres de columnas en curvas
            for curva in curvas:
                curva['columna_original'] = curva['columna']
                curva['columna'] = f"{curva['columna']}_NORM"
                curva['etiqueta'] = f"{curva['etiqueta']} (Norm)"
        
        else:
            # Normalizar todo el dataset
            for curva in curvas:
                col_data = df_procesado[curva['columna']].dropna()
                if len(col_data) > 0:
                    if metodo_normalizacion == "Z-score":
                        df_procesado[f"{curva['columna']}_NORM"] = (col_data - col_data.mean()) / col_data.std()
                    elif metodo_normalizacion == "Min-Max":
                        df_procesado[f"{curva['columna']}_NORM"] = (col_data - col_data.min()) / (col_data.max() - col_data.min())
                    elif metodo_normalizacion == "Robust (IQR)":
                        Q1 = col_data.quantile(0.25)
                        Q3 = col_data.quantile(0.75)
                        IQR = Q3 - Q1
                        df_procesado[f"{curva['columna']}_NORM"] = (col_data - col_data.median()) / IQR
                    elif metodo_normalizacion == "Percentil (5-95%)":
                        p5 = col_data.quantile(0.05)
                        p95 = col_data.quantile(0.95)
                        df_procesado[f"{curva['columna']}_NORM"] = (col_data - p5) / (p95 - p5)
            
            # Actualizar nombres de columnas en curvas
            for curva in curvas:
                curva['columna_original'] = curva['columna']
                curva['columna'] = f"{curva['columna']}_NORM"
                curva['etiqueta'] = f"{curva['etiqueta']} (Norm)"
    else:
        df_procesado = df_temp.copy()

    # Continuar con el procesamiento normal
    df_comun = df_procesado[["PROFUNDIDAD"]].copy()
    for curva in curvas:
        col_data = df_procesado[["PROFUNDIDAD", curva["columna"]]].dropna()
        col_data = col_data.rename(columns={curva["columna"]: curva["nombre_interno"]})
        df_comun = df_comun.merge(col_data, on="PROFUNDIDAD", how="inner")

    if len(df_comun) == 0:
        st.error("âŒ No hay datos comunes en profundidad para todas las curvas seleccionadas.")
        return

    profundidad_comun = df_comun["PROFUNDIDAD"].values

    # --- FunciÃ³n de suavizado ---
    def suavizar_serie(serie, ventana):
        return serie.rolling(window=ventana, center=True, min_periods=1).mean()

    # ================================
    # ðŸ“Š ANÃLISIS DE CORRELACIÃ“N
    # ================================
    st.subheader("ðŸ“Š AnÃ¡lisis de CorrelaciÃ³n")
    
    if len(curvas) >= 2:
        col_corr1, col_corr2 = st.columns(2)
        
        with col_corr1:
            st.markdown("**Selecciona dos curvas para analizar correlaciÃ³n:**")
            curva1_corr = st.selectbox(
                "Curva 1:",
                options=[c["columna"] for c in curvas],
                key="curva1_corr"
            )
            curva2_corr = st.selectbox(
                "Curva 2:",
                options=[c["columna"] for c in curvas if c["columna"] != curva1_corr],
                key="curva2_corr"
            )
            
            # Calcular correlaciÃ³n
            if curva1_corr and curva2_corr:
                datos_curva1 = df_comun[next(c["nombre_interno"] for c in curvas if c["columna"] == curva1_corr)]
                datos_curva2 = df_comun[next(c["nombre_interno"] for c in curvas if c["columna"] == curva2_corr)]
                
                # Filtrar datos vÃ¡lidos
                mask = (~datos_curva1.isna()) & (~datos_curva2.isna())
                datos_curva1_clean = datos_curva1[mask]
                datos_curva2_clean = datos_curva2[mask]
                
                if len(datos_curva1_clean) > 1:
                    # Calcular coeficientes
                    correlacion_pearson, p_value_pearson = stats.pearsonr(datos_curva1_clean, datos_curva2_clean)
                    r_cuadrado = correlacion_pearson ** 2
                    
                    # Mostrar resultados
                    st.metric("Coeficiente de CorrelaciÃ³n (Pearson)", f"{correlacion_pearson:.4f}")
                    st.metric("RÂ² (Coeficiente de determinaciÃ³n)", f"{r_cuadrado:.4f}")
                    st.metric("Valor p", f"{p_value_pearson:.4e}")
                    
                    # InterpretaciÃ³n
                    if abs(correlacion_pearson) > 0.7:
                        st.success("âœ… **Fuerte correlaciÃ³n** entre las curvas")
                    elif abs(correlacion_pearson) > 0.3:
                        st.info("ðŸŸ¡ **CorrelaciÃ³n moderada** entre las curvas")
                    else:
                        st.warning("ðŸŸ  **CorrelaciÃ³n dÃ©bil** entre las curvas")
        
        with col_corr2:
            # GrÃ¡fico de dispersiÃ³n para correlaciÃ³n
            if curva1_corr and curva2_corr and len(datos_curva1_clean) > 1:
                fig_dispersion = px.scatter(
                    x=datos_curva1_clean,
                    y=datos_curva2_clean,
                    title=f"DispersiÃ³n: {curva1_corr} vs {curva2_corr}",
                    labels={'x': curva1_corr, 'y': curva2_corr}
                )
                fig_dispersion.update_layout(height=400)
                st.plotly_chart(fig_dispersion, use_container_width=True)

    # ================================
    # ðŸ“ˆ GRÃFICO PRINCIPAL (ACTUALIZADO CON MARCADORES Y NOMBRE PERSONALIZADO)
    # ================================
    st.subheader("ðŸ“ˆ GrÃ¡fico de Registros")

    # ConfiguraciÃ³n del grÃ¡fico
    col_config1, col_config2 = st.columns(2)
    
    with col_config1:
        altura_grafico = st.slider("Altura del grÃ¡fico (pÃ­xeles)", 400, 1200, 600)
        mostrar_grid = st.checkbox("Mostrar grid", value=True)
        mostrar_leyenda = st.checkbox("Mostrar leyenda", value=True)
        mostrar_marcadores = st.checkbox("Mostrar marcadores estratigrÃ¡ficos", value=True) if agregar_marcadores else False
        
    with col_config2:
        escala_log = st.checkbox("Usar escala logarÃ­tmica para curvas seleccionadas")
        mostrar_puntos = st.checkbox("Mostrar puntos en curvas", value=False)
        mostrar_nombres_marcadores = st.checkbox("Mostrar nombres en marcadores", value=True) if agregar_marcadores else False

    # Crear figura
    fig, ax1 = plt.subplots(figsize=(12, altura_grafico/100))
    
    # Configurar tÃ­tulo personalizado
    if nombre_grafica:
        plt.title(nombre_grafica, fontsize=14, fontweight='bold', pad=20)
    
    # Configurar eje Y (profundidad)
    ax1.set_ylabel("Profundidad")
    if invertir_eje_y:
        ax1.set_ylim(profundidad_max, profundidad_min)
    else:
        ax1.set_ylim(profundidad_min, profundidad_max)

    # Crear eje secundario si es necesario
    ax2 = None
    tiene_eje_secundario = any(curva["eje_secundario"] for curva in curvas)
    
    if tiene_eje_secundario:
        ax2 = ax1.twiny()

    # Graficar cada curva
    for curva in curvas:
        datos_curva = df_comun[curva["nombre_interno"]].values
        
        # Aplicar suavizado si estÃ¡ activado
        if curva["suavizar"] and curva["ventana"] > 1:
            datos_curva = suavizar_serie(pd.Series(datos_curva), curva["ventana"]).values
        
        # Aplicar inversiÃ³n si estÃ¡ activado
        if curva["invertir_x"]:
            datos_curva = -datos_curva
        
        # Seleccionar eje
        eje_actual = ax2 if curva["eje_secundario"] and ax2 is not None else ax1
        
        # Graficar
        line = eje_actual.plot(
            datos_curva,
            profundidad_comun,
            color=curva["color"],
            linestyle=curva["estilo"],
            label=curva["etiqueta"],
            zorder=curva["zorder"],
            linewidth=1.5,
            marker='o' if mostrar_puntos else None,
            markersize=3 if mostrar_puntos else 0
        )[0]
        
        # Configurar escala logarÃ­tmica si estÃ¡ activada
        if escala_log:
            eje_actual.set_xscale('log')

    # ================================
    # ðŸŽ¯ AGREGAR MARCADORES ESTRATIGRÃFICOS AL GRÃFICO
    # ================================
    if agregar_marcadores and mostrar_marcadores:
        for i, marcador in enumerate(marcadores):
            # Agregar lÃ­nea horizontal para cada marcador
            ax1.axhline(
                y=marcador["profundidad"], 
                color=marcador["color"], 
                linestyle=marcador["estilo"],
                linewidth=2,
                alpha=0.8,
                zorder=10  # Alto zorder para que estÃ© por encima de las curvas
            )
            
            # Agregar etiqueta si estÃ¡ activado
            if mostrar_nombres_marcadores:
                ax1.text(
                    0.02,  # PosiciÃ³n x (2% del ancho del grÃ¡fico)
                    marcador["profundidad"], 
                    f"  {marcador['nombre']}",
                    verticalalignment='center',
                    horizontalalignment='left',
                    transform=ax1.get_yaxis_transform(),  # Usar transformaciÃ³n del eje y
                    fontsize=10,
                    fontweight='bold',
                    color=marcador["color"],
                    bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8, edgecolor=marcador["color"])
                )

    # Configurar ejes y leyenda
    ax1.set_xlabel("Curvas principales")
    if ax2 is not None:
        ax2.set_xlabel("Curvas secundarias")
    
    if mostrar_leyenda:
        ax1.legend(loc='upper left', bbox_to_anchor=(1, 1))
        if ax2 is not None:
            ax2.legend(loc='upper right', bbox_to_anchor=(1, 1))
    
    if mostrar_grid:
        ax1.grid(True, alpha=0.3)
        if ax2 is not None:
            ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    st.pyplot(fig)

    # ================================
    # ðŸ“‹ TABLA DE MARCADORES ESTRATIGRÃFICOS
    # ================================
    if agregar_marcadores and marcadores:
        st.subheader("ðŸ“‹ Resumen de Marcadores EstratigrÃ¡ficos")
        
        # Crear tabla de marcadores
        marcadores_df = pd.DataFrame(marcadores)
        st.dataframe(marcadores_df, use_container_width=True)
        
        # Mostrar informaciÃ³n adicional si hay anotaciones
        marcadores_con_anotaciones = [m for m in marcadores if m["anotacion"]]
        if marcadores_con_anotaciones:
            st.markdown("#### ðŸ“ Anotaciones de Formaciones")
            for marcador in marcadores_con_anotaciones:
                st.markdown(f"**{marcador['nombre']}** ({marcador['profundidad']} m): {marcador['anotacion']}")

    # ================================
    # ðŸ’¾ EXPORTACIÃ“N PROFESIONAL MEJORADA
    # ================================
    st.subheader("ðŸ’¾ ExportaciÃ³n Profesional")
    
    col_exp1, col_exp2, col_exp3, col_exp4 = st.columns(4)
    
    with col_exp1:
        # Exportar grÃ¡fico como PNG de alta calidad
        buf_png = BytesIO()
        fig.savefig(buf_png, format='png', dpi=300, bbox_inches='tight')
        st.download_button(
            label="ðŸ“¥ PNG Alta Calidad",
            data=buf_png.getvalue(),
            file_name=f"{nombre_grafica}_{datetime.now().strftime('%Y%m%d_%H%M')}.png",
            mime="image/png",
            use_container_width=True
        )
    
    with col_exp2:
        # Exportar como PDF para reportes
        buf_pdf = BytesIO()
        fig.savefig(buf_pdf, format='pdf', dpi=300, bbox_inches='tight')
        st.download_button(
            label="ðŸ“„ PDF para Reportes",
            data=buf_pdf.getvalue(),
            file_name=f"{nombre_grafica}_{datetime.now().strftime('%Y%m%d_%H%M')}.pdf",
            mime="application/pdf",
            use_container_width=True
        )
    
    with col_exp3:
        # Exportar datos procesados en Excel
        output_excel = BytesIO()
        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:
            # Hoja principal de datos
            df_comun.to_excel(writer, sheet_name='Datos_Procesados', index=False)
            
            # Hoja de configuraciÃ³n de curvas
            config_data = []
            for curva in curvas:
                config_row = {
                    'Columna': curva['columna'],
                    'Etiqueta': curva['etiqueta'],
                    'Color': curva['color'],
                    'Estilo_Linea': curva['estilo'],
                    'Eje_Secundario': curva['eje_secundario'],
                    'Suavizado': curva['suavizar'],
                    'Ventana_Suavizado': curva['ventana'],
                    'Invertir_Eje_X': curva['invertir_x']
                }
                if aplicar_normalizacion:
                    config_row['Columna_Original'] = curva.get('columna_original', curva['columna'])
                    config_row['Metodo_Normalizacion'] = metodo_normalizacion
                config_data.append(config_row)
            
            config_df = pd.DataFrame(config_data)
            config_df.to_excel(writer, sheet_name='Configuracion_Curvas', index=False)
            
            # Hoja con marcadores estratigrÃ¡ficos si existen
            if agregar_marcadores and marcadores:
                marcadores_export = []
                for marcador in marcadores:
                    marcadores_export.append({
                        'Nombre_Formacion': marcador['nombre'],
                        'Profundidad': marcador['profundidad'],
                        'Color': marcador['color'],
                        'Estilo_Linea': marcador['estilo'],
                        'Anotacion': marcador['anotacion']
                    })
                marcadores_df = pd.DataFrame(marcadores_export)
                marcadores_df.to_excel(writer, sheet_name='Marcadores_Estratigraficos', index=False)
            
            # Hoja de metadatos
            metadata = {
                'Campo': [nombre_grafica],
                'Fecha_Generacion': [datetime.now().strftime('%Y-%m-%d %H:%M')],
                'Plantilla_Utilizada': [plantilla_seleccionada],
                'Rango_Profundidad': [f"{profundidad_min} - {profundidad_max}"],
                'Normalizacion_Aplicada': [metodo_normalizacion if aplicar_normalizacion else 'Ninguna'],
                'Numero_Curvas': [len(curvas)],
                'Numero_Marcadores': [len(marcadores) if agregar_marcadores else 0]
            }
            metadata_df = pd.DataFrame(metadata)
            metadata_df.to_excel(writer, sheet_name='Metadatos', index=False)
        
        st.download_button(
            label="ðŸ“Š Excel Completo",
            data=output_excel.getvalue(),
            file_name=f"{nombre_grafica}_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
    
    with col_exp4:
        # Exportar configuraciÃ³n como template reutilizable
        config_str = f"# CONFIGURACIÃ“N DE VISUALIZACIÃ“N - {nombre_grafica}\n"
        config_str += f"# Generado: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"
        
        config_str += f"PLANTILLA: {plantilla_seleccionada}\n"
        config_str += f"RANGO_PROFUNDIDAD: {profundidad_min},{profundidad_max}\n"
        config_str += f"NORMALIZACION: {metodo_normalizacion if aplicar_normalizacion else 'NINGUNA'}\n"
        config_str += f"INVERTIR_EJE_Y: {invertir_eje_y}\n\n"
        
        config_str += "## CURVAS CONFIGURADAS:\n"
        for i, curva in enumerate(curvas):
            config_str += f"CURVA_{i+1}: {curva['columna']}\n"
            config_str += f"  ETIQUETA: {curva['etiqueta']}\n"
            config_str += f"  COLOR: {curva['color']}\n"
            config_str += f"  ESTILO: {curva['estilo']}\n"
            config_str += f"  EJE_SECUNDARIO: {curva['eje_secundario']}\n"
            config_str += f"  SUAVIZADO: {curva['suavizar']}\n"
            if curva['suavizar']:
                config_str += f"  VENTANA: {curva['ventana']}\n"
            config_str += f"  INVERTIR_X: {curva['invertir_x']}\n\n"
        
        if agregar_marcadores and marcadores:
            config_str += "## MARCADORES ESTRATIGRÃFICOS:\n"
            for i, marcador in enumerate(marcadores):
                config_str += f"MARCADOR_{i+1}: {marcador['nombre']}\n"
                config_str += f"  PROFUNDIDAD: {marcador['profundidad']}\n"
                config_str += f"  COLOR: {marcador['color']}\n"
                config_str += f"  ESTILO: {marcador['estilo']}\n"
                if marcador['anotacion']:
                    config_str += f"  ANOTACION: {marcador['anotacion']}\n"
                config_str += "\n"
        
        st.download_button(
            label="âš™ï¸ Template Config",
            data=config_str.encode(),
            file_name=f"template_{nombre_grafica}_{datetime.now().strftime('%Y%m%d_%H%M')}.cfg",
            mime="text/plain",
            use_container_width=True
        )

    # ================================
    # ðŸ“Š COMPARACIÃ“N MULTI-POZO (VISUALIZACIÃ“N AVANZADA)
    # ================================
    if es_multicarga and len(archivos_unicos) > 1:
        st.subheader("ðŸ“Š ComparaciÃ³n Multi-pozo")
        
        col_comp1, col_comp2 = st.columns(2)
        
        with col_comp1:
            mostrar_comparacion = st.checkbox(
                "Mostrar comparaciÃ³n side-by-side",
                value=False,
                help="Mostrar grÃ¡ficas individuales para cada pozo"
            )
        
        with col_comp2:
            if mostrar_comparacion:
                pozos_comparar = st.multiselect(
                    "Seleccionar pozos para comparar:",
                    options=archivos_unicos,
                    default=archivos_unicos[:min(3, len(archivos_unicos))]
                )
        
        if mostrar_comparacion and pozos_comparar:
            st.markdown("#### ðŸ“ˆ Vista Comparativa")
            
            # Crear subplots para comparaciÃ³n
            n_pozos = len(pozos_comparar)
            fig_comparacion, axes = plt.subplots(1, n_pozos, figsize=(5*n_pozos, altura_grafico/100))
            if n_pozos == 1:
                axes = [axes]
            
            for idx, pozo in enumerate(pozos_comparar):
                df_pozo = df_temp[df_temp['ARCHIVO_ORIGEN'] == pozo]
                ax = axes[idx]
                
                # Configurar eje Y
                if invertir_eje_y:
                    ax.set_ylim(profundidad_max, profundidad_min)
                else:
                    ax.set_ylim(profundidad_min, profundidad_max)
                
                # Graficar curvas para este pozo
                for curva in curvas:
                    if curva['columna'] in df_pozo.columns:
                        datos_curva = df_pozo[curva['columna']].values
                        prof_pozo = df_pozo['PROFUNDIDAD'].values
                        
                        # Aplicar suavizado
                        if curva["suavizar"] and curva["ventana"] > 1:
                            datos_curva = suavizar_serie(pd.Series(datos_curva), curva["ventana"]).values
                        
                        # Aplicar inversiÃ³n
                        if curva["invertir_x"]:
                            datos_curva = -datos_curva
                        
                        ax.plot(
                            datos_curva,
                            prof_pozo,
                            color=curva["color"],
                            linestyle=curva["estilo"],
                            label=curva["etiqueta"],
                            linewidth=1.5
                        )
                
                ax.set_title(f"{pozo}", fontsize=10)
                ax.grid(True, alpha=0.3)
                if idx == 0:
                    ax.set_ylabel("Profundidad")
                ax.set_xlabel("Valores")
            
            plt.tight_layout()
            st.pyplot(fig_comparacion)
# ================================
# ðŸ” MÃ“DULO 2: ANÃLISIS ESTADÃSTICO (SIMPLIFICADO)
# ================================
def modulo_analisis_estadistico():
    """MÃ³dulo de anÃ¡lisis estadÃ­stico mejorado"""
    
    instrucciones_estadisticas = """
    1. **Selecciona las columnas** numÃ©ricas para anÃ¡lisis
    2. **Configura** los parÃ¡metros estadÃ­sticos
    3. **Analiza** distribuciones y correlaciones
    4. **Exporta** reportes estadÃ­sticos completos
    """
    
    df = mostrar_cargador_datos(
        "AnÃ¡lisis EstadÃ­stico", 
        instrucciones_estadisticas,
        permitir_multicarga=True
    )
    
    if df is None:
        return
    
    st.header("ðŸ“Š AnÃ¡lisis EstadÃ­stico Avanzado")
    
    # Verificar si es multicarga
    es_multicarga = 'ARCHIVO_ORIGEN' in df.columns if df is not None else False
    
    if es_multicarga:
        st.info("ðŸŽ¯ **Modo Multicarga Activado** - Analizando datos combinados")
        
        # EstadÃ­sticas por archivo
        st.subheader("ðŸ“ˆ EstadÃ­sticas por Archivo")
        stats_por_archivo = df.groupby('ARCHIVO_ORIGEN').agg({
            'ARCHIVO_ORIGEN': 'count'
        }).rename(columns={'ARCHIVO_ORIGEN': 'Filas'})
        
        st.dataframe(stats_por_archivo, use_container_width=True)
    
    # SelecciÃ³n de columnas
    columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not columnas_numericas:
        st.error("âŒ No se encontraron columnas numÃ©ricas para anÃ¡lisis")
        return
    
    st.subheader("ðŸŽ¯ SelecciÃ³n de Variables")
    
    columnas_analisis = st.multiselect(
        "Selecciona las columnas para anÃ¡lisis:",
        options=columnas_numericas,
        default=columnas_numericas[:min(5, len(columnas_numericas))]
    )
    
    if not columnas_analisis:
        st.warning("âš ï¸ Selecciona al menos una columna para anÃ¡lisis")
        return
    
    # EstadÃ­sticas descriptivas
    st.subheader("ðŸ“‹ EstadÃ­sticas Descriptivas")
    stats_df = df[columnas_analisis].describe().T
    stats_df['Varianza'] = df[columnas_analisis].var()
    stats_df['AsimetrÃ­a'] = df[columnas_analisis].skew()
    stats_df['Curtosis'] = df[columnas_analisis].kurtosis()
    
    st.dataframe(stats_df.style.format("{:.4f}"), use_container_width=True)
    
    # Matriz de correlaciÃ³n
    if len(columnas_analisis) > 1:
        st.subheader("ðŸ”— Matriz de CorrelaciÃ³n")
        corr_matrix = df[columnas_analisis].corr()
        
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax, fmt='.3f')
        st.pyplot(fig)
# ================================
# ðŸ›¢ï¸ MÃ“DULO 3: ANÃLISIS PETROFÃSICO
# ================================
def modulo_analisis_petrofisico():
    st.header("ðŸ›¢ï¸ AnÃ¡lisis PetrofÃ­sico Avanzado")
    
    # Cargar datos
    df = mostrar_cargador_datos(
        "AnÃ¡lisis PetrofÃ­sico", 
        "Carga datos con curvas de registros para anÃ¡lisis petrofÃ­sico (RESISTIVIDAD, POROSIDAD, etc.)",
        permitir_multicarga=True
    )
    
    if df is None:
        return

    st.subheader("ðŸŽ¯ ConfiguraciÃ³n de AnÃ¡lisis PetrofÃ­sico")

    # SelecciÃ³n de curvas disponibles
    columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not columnas_numericas:
        st.error("âŒ No se encontraron columnas numÃ©ricas para anÃ¡lisis")
        return

    # ================================
    # ðŸ“Š CONFIGURACIÃ“N DE CURVAS
    # ================================
    col_curvas1, col_curvas2 = st.columns(2)
    
    with col_curvas1:
        st.markdown("#### ðŸ“ˆ Curvas de Resistividad")
        col_rt = st.selectbox("Resistividad Profunda (RT):", options=columnas_numericas, index=0)
        col_rs = st.selectbox("Resistividad Someras (RS):", options=columnas_numericas, index=min(1, len(columnas_numericas)-1))
        
        st.markdown("#### ðŸ“Š Curvas de Porosidad")
        col_density = st.selectbox("Densidad (RHOB):", options=columnas_numericas, index=min(2, len(columnas_numericas)-1))
        col_neutron = st.selectbox("NeutrÃ³n (NPHI):", options=columnas_numericas, index=min(3, len(columnas_numericas)-1))
        col_sonic = st.selectbox("SÃ³nico (DT):", options=columnas_numericas, index=min(4, len(columnas_numericas)-1))
    
    with col_curvas2:
        st.markdown("#### ðŸŽ¯ ParÃ¡metros PetrofÃ­sicos")
        rw = st.number_input("Resistividad del Agua (Rw) [Î©m]:", value=0.1, min_value=0.001, max_value=10.0, step=0.01)
        a_value = st.number_input("Constante a (Archie):", value=1.0, min_value=0.1, max_value=2.0, step=0.1)
        m_value = st.number_input("Exponente de cementaciÃ³n m:", value=2.0, min_value=1.5, max_value=3.0, step=0.1)
        n_value = st.number_input("Exponente de saturaciÃ³n n:", value=2.0, min_value=1.5, max_value=3.0, step=0.1)
        
        st.markdown("#### âš™ï¸ ConfiguraciÃ³n de LitologÃ­as")
        vshale_cutoff = st.slider("Cutoff de Lutita (Vshale):", min_value=0.0, max_value=1.0, value=0.5, step=0.05)
        sw_cutoff = st.slider("Cutoff de SaturaciÃ³n de Agua (Sw):", min_value=0.0, max_value=1.0, value=0.6, step=0.05)

    # ================================
    # ðŸ§® CÃLCULOS PETROFÃSICOS
    # ================================
    st.subheader("ðŸ§® CÃ¡lculos PetrofÃ­sicos")
    
    # Crear copia para cÃ¡lculos
    df_petro = df.copy()
    
    # Inicializar columnas de resultados
    calculos_realizados = []
    
    # 1. CÃ¡lculo de Porosidad
    if st.checkbox("Calcular Porosidad a partir de Densidad", value=True):
        if col_density in df_petro.columns:
            # ParÃ¡metros de matriz y fluido
            rhoma = st.number_input("Densidad de Matriz (g/cc):", value=2.65, min_value=2.0, max_value=3.0, step=0.05)
            rhof = st.number_input("Densidad de Fluido (g/cc):", value=1.0, min_value=0.8, max_value=1.2, step=0.05)
            
            df_petro['PHI_DENS'] = (rhoma - df_petro[col_density]) / (rhoma - rhof)
            df_petro['PHI_DENS'] = df_petro['PHI_DENS'].clip(0, 1)  # Limitar entre 0 y 1
            calculos_realizados.append('PHI_DENS')
    
    # 2. CÃ¡lculo de SaturaciÃ³n de Agua (Archie)
    if st.checkbox("Calcular SaturaciÃ³n de Agua (EcuaciÃ³n de Archie)", value=True):
        if all(col in df_petro.columns for col in [col_rt]) and 'PHI_DENS' in df_petro.columns:
            # Calcular Ro (resistividad de formaciÃ³n 100% saturada con agua)
            df_petro['RO'] = a_value * rw / (df_petro['PHI_DENS'] ** m_value)
            
            # Calcular Sw (saturaciÃ³n de agua)
            df_petro['SW_ARCHIE'] = ((a_value * rw) / (df_petro[col_rt] * df_petro['PHI_DENS'] ** m_value)) ** (1/n_value)
            df_petro['SW_ARCHIE'] = df_petro['SW_ARCHIE'].clip(0, 1)  # Limitar entre 0 y 1
            
            # Calcular Hidrocarburo mÃ³vil
            df_petro['SHC_ARCHIE'] = 1 - df_petro['SW_ARCHIE']
            df_petro['SHC_ARCHIE'] = df_petro['SHC_ARCHIE'].clip(0, 1)
            
            calculos_realizados.extend(['SW_ARCHIE', 'SHC_ARCHIE'])

    # 3. CÃ¡lculo de Volumen de Lutita
    if st.checkbox("Calcular Volumen de Lutita", value=True):
        # Usar curva de rayos gamma si estÃ¡ disponible
        col_gamma = st.selectbox("Curva de Rayos Gamma (GR):", 
                               options=[''] + columnas_numericas,
                               help="Selecciona la curva de rayos gamma para cÃ¡lculo de Vshale")
        
        if col_gamma and col_gamma in df_petro.columns:
            # Obtener valores de GR limpio y GR lutita
            gr_min = st.number_input("GR Min (arena limpia):", 
                                   value=float(df_petro[col_gamma].quantile(0.05)), 
                                   step=1.0)
            gr_max = st.number_input("GR Max (lutita):", 
                                   value=float(df_petro[col_gamma].quantile(0.95)), 
                                   step=1.0)
            
            # Calcular Vshale (mÃ©todo lineal)
            df_petro['VSHALE'] = (df_petro[col_gamma] - gr_min) / (gr_max - gr_min)
            df_petro['VSHALE'] = df_petro['VSHALE'].clip(0, 1)
            calculos_realizados.append('VSHALE')

    # ================================
    # ðŸ“ˆ VISUALIZACIÃ“N DE RESULTADOS
    # ================================
    if calculos_realizados:
        st.subheader("ðŸ“ˆ Resultados del AnÃ¡lisis PetrofÃ­sico")
        
        # Mostrar estadÃ­sticas de los cÃ¡lculos
        st.markdown("#### ðŸ“Š EstadÃ­sticas de los CÃ¡lculos")
        stats_petro = df_petro[calculos_realizados].describe()
        st.dataframe(stats_petro, use_container_width=True)
        
        # GrÃ¡ficos de resultados
        col_viz1, col_viz2 = st.columns(2)
        
        with col_viz1:
            # Histograma de porosidad
            if 'PHI_DENS' in calculos_realizados:
                fig_phi, ax_phi = plt.subplots(figsize=(8, 6))
                ax_phi.hist(df_petro['PHI_DENS'].dropna(), bins=30, alpha=0.7, color='lightblue', edgecolor='black')
                ax_phi.set_xlabel('Porosidad (PHI_DENS)')
                ax_phi.set_ylabel('Frecuencia')
                ax_phi.set_title('DistribuciÃ³n de Porosidad')
                ax_phi.grid(True, alpha=0.3)
                st.pyplot(fig_phi)
                plt.close(fig_phi)
        
        with col_viz2:
            # Histograma de saturaciÃ³n de agua
            if 'SW_ARCHIE' in calculos_realizados:
                fig_sw, ax_sw = plt.subplots(figsize=(8, 6))
                ax_sw.hist(df_petro['SW_ARCHIE'].dropna(), bins=30, alpha=0.7, color='lightcoral', edgecolor='black')
                ax_sw.set_xlabel('SaturaciÃ³n de Agua (SW_ARCHIE)')
                ax_sw.set_ylabel('Frecuencia')
                ax_sw.set_title('DistribuciÃ³n de SaturaciÃ³n de Agua')
                ax_sw.grid(True, alpha=0.3)
                st.pyplot(fig_sw)
                plt.close(fig_sw)
        
        # Crossplot Porosidad vs Resistividad
        if all(col in calculos_realizados for col in ['PHI_DENS', 'SW_ARCHIE']):
            st.markdown("#### ðŸ“Š Crossplot Porosidad vs SaturaciÃ³n de Agua")
            fig_cross, ax_cross = plt.subplots(figsize=(10, 6))
            
            scatter = ax_cross.scatter(df_petro['PHI_DENS'], df_petro['SW_ARCHIE'], 
                                     c=df_petro[col_rt] if col_rt in df_petro.columns else None,
                                     alpha=0.6, s=30, cmap='viridis')
            ax_cross.set_xlabel('Porosidad (PHI_DENS)')
            ax_cross.set_ylabel('SaturaciÃ³n de Agua (SW_ARCHIE)')
            ax_cross.set_title('Porosidad vs SaturaciÃ³n de Agua')
            ax_cross.grid(True, alpha=0.3)
            
            if col_rt in df_petro.columns:
                plt.colorbar(scatter, ax=ax_cross, label='Resistividad (RT)')
            
            st.pyplot(fig_cross)
            plt.close(fig_cross)

        # ================================
        # ðŸŽ¯ IDENTIFICACIÃ“N DE ZONAS INTERESANTES
        # ================================
        st.subheader("ðŸŽ¯ IdentificaciÃ³n de Zonas Potenciales")
        
        # Definir criterios para zona interesante
        criterios_zona = {
            'Porosidad mÃ­nima': st.number_input("Porosidad mÃ­nima:", value=0.1, min_value=0.0, max_value=0.5, step=0.01),
            'SaturaciÃ³n mÃ¡xima de agua': st.number_input("SaturaciÃ³n mÃ¡xima de agua:", value=0.5, min_value=0.0, max_value=1.0, step=0.05),
            'Volumen mÃ¡ximo de lutita': st.number_input("Volumen mÃ¡ximo de lutita:", value=0.3, min_value=0.0, max_value=1.0, step=0.05)
        }
        
        # Aplicar criterios
        mascara_zona = pd.Series(True, index=df_petro.index)
        
        if 'PHI_DENS' in calculos_realizados:
            mascara_zona &= (df_petro['PHI_DENS'] >= criterios_zona['Porosidad mÃ­nima'])
        
        if 'SW_ARCHIE' in calculos_realizados:
            mascara_zona &= (df_petro['SW_ARCHIE'] <= criterios_zona['SaturaciÃ³n mÃ¡xima de agua'])
        
        if 'VSHALE' in calculos_realizados:
            mascara_zona &= (df_petro['VSHALE'] <= criterios_zona['Volumen mÃ¡ximo de lutita'])
        
        zonas_potenciales = df_petro[mascara_zona]
        
        st.success(f"**Zonas potenciales identificadas:** {len(zonas_potenciales)} intervalos")
        
        if not zonas_potenciales.empty:
            st.dataframe(zonas_potenciales[calculos_realizados].describe(), use_container_width=True)

        # ================================
        # ðŸ’¾ EXPORTAR RESULTADOS
        # ================================
        st.subheader("ðŸ’¾ Exportar Resultados PetrofÃ­sicos")
        
        if st.button("ðŸ“¥ Generar Reporte PetrofÃ­sico", use_container_width=True):
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                # Hoja de datos originales + cÃ¡lculos
                df_petro.to_excel(writer, sheet_name='Datos_Petrofisicos', index=False)
                
                # Hoja de zonas potenciales
                if not zonas_potenciales.empty:
                    zonas_potenciales.to_excel(writer, sheet_name='Zonas_Potenciales', index=False)
                
                # Hoja de parÃ¡metros y configuraciÃ³n
                config_data = {
                    'ParÃ¡metro': ['Rw', 'a', 'm', 'n', 'Vshale Cutoff', 'Sw Cutoff', 'Zonas Identificadas'],
                    'Valor': [rw, a_value, m_value, n_value, vshale_cutoff, sw_cutoff, len(zonas_potenciales)]
                }
                config_df = pd.DataFrame(config_data)
                config_df.to_excel(writer, sheet_name='Configuracion', index=False)
            
            output.seek(0)
            
            st.download_button(
                label="â¬‡ï¸ Descargar Reporte PetrofÃ­sico",
                data=output,
                file_name=f"analisis_petrofisico_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
    else:
        st.warning("âš ï¸ No se han realizado cÃ¡lculos. Activa al menos una opciÃ³n de cÃ¡lculo arriba.")

# ================================
# ðŸ” MÃ“DULO 4: INTERPRETACIÃ“N AVANZADA
# ================================
def modulo_interpretacion_avanzada():
    st.header("ðŸ” InterpretaciÃ³n Avanzada")
    
    # Cargar datos
    df = mostrar_cargador_datos(
        "InterpretaciÃ³n Avanzada", 
        "Carga datos para anÃ¡lisis de facies, clusterizaciÃ³n y interpretaciÃ³n avanzada",
        permitir_multicarga=True
    )
    
    if df is None:
        return

    st.subheader("ðŸŽ¯ ConfiguraciÃ³n de InterpretaciÃ³n")

    # SelecciÃ³n de variables para anÃ¡lisis
    columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not columnas_numericas:
        st.error("âŒ No se encontraron columnas numÃ©ricas para anÃ¡lisis")
        return

    st.markdown("#### ðŸ“Š SelecciÃ³n de Variables para AnÃ¡lisis")
    variables_analisis = st.multiselect(
        "Selecciona las variables para el anÃ¡lisis:",
        options=columnas_numericas,
        default=columnas_numericas[:min(4, len(columnas_numericas))],
        help="Selecciona las curvas que representen diferentes propiedades de la formaciÃ³n"
    )

    if not variables_analisis:
        st.warning("âš ï¸ Por favor selecciona al menos 2 variables para anÃ¡lisis")
        return

    if len(variables_analisis) < 2:
        st.error("âŒ Necesitas al menos 2 variables para anÃ¡lisis de clusterizaciÃ³n")
        return

    # Preparar datos
    df_analysis = df[variables_analisis].dropna()
    
    if df_analysis.empty:
        st.error("âŒ No hay datos vÃ¡lidos despuÃ©s de eliminar valores nulos")
        return

    # ================================
    # ðŸŽ¯ ANÃLISIS DE CLUSTERIZACIÃ“N
    # ================================
    st.subheader("ðŸŽ¯ AnÃ¡lisis de ClusterizaciÃ³n")
    
    col_cluster1, col_cluster2 = st.columns(2)
    
    with col_cluster1:
        n_clusters = st.slider("NÃºmero de clusters:", min_value=2, max_value=10, value=3)
        algoritmo = st.selectbox("Algoritmo de clusterizaciÃ³n:", 
                               options=['KMeans', 'DBSCAN', 'Agglomerative'])
    
    with col_cluster2:
        normalizar = st.checkbox("Normalizar datos antes de clusterizaciÃ³n", value=True)
        random_state = st.number_input("Semilla aleatoria:", value=42, min_value=0, max_value=100)

    # Normalizar datos si es necesario
    if normalizar:
        scaler = StandardScaler()
        datos_escalados = scaler.fit_transform(df_analysis)
    else:
        datos_escalados = df_analysis.values

    # Aplicar clusterizaciÃ³n
    try:
        if algoritmo == 'KMeans':
            clusterer = KMeans(n_clusters=n_clusters, random_state=random_state)
            labels = clusterer.fit_predict(datos_escalados)
            
        elif algoritmo == 'DBSCAN':
            eps = st.slider("ParÃ¡metro EPS (DBSCAN):", min_value=0.1, max_value=2.0, value=0.5, step=0.1)
            min_samples = st.slider("MÃ­nimo de muestras (DBSCAN):", min_value=2, max_value=20, value=5)
            clusterer = DBSCAN(eps=eps, min_samples=min_samples)
            labels = clusterer.fit_predict(datos_escalados)
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            
        else:  # Agglomerative
            from sklearn.cluster import AgglomerativeClustering
            clusterer = AgglomerativeClustering(n_clusters=n_clusters)
            labels = clusterer.fit_predict(datos_escalados)
        
        # AÃ±adir labels al DataFrame
        df_analysis_clustered = df_analysis.copy()
        df_analysis_clustered['CLUSTER'] = labels
        df_analysis_clustered['CLUSTER'] = df_analysis_clustered['CLUSTER'].astype(str)
        
        st.success(f"âœ… ClusterizaciÃ³n completada: {n_clusters} clusters identificados")
        
    except Exception as e:
        st.error(f"âŒ Error en clusterizaciÃ³n: {str(e)}")
        return

    # ================================
    # ðŸ“Š VISUALIZACIÃ“N DE CLUSTERS
    # ================================
    st.subheader("ðŸ“Š VisualizaciÃ³n de Clusters")
    
    # Scatter plot matrix
    if len(variables_analisis) >= 2:
        col_viz1, col_viz2 = st.columns(2)
        
        with col_viz1:
            x_var = st.selectbox("Variable X:", options=variables_analisis, key="cluster_x")
        with col_viz2:
            y_var = st.selectbox("Variable Y:", options=variables_analisis, key="cluster_y")
        
        if x_var != y_var:
            fig_scatter, ax_scatter = plt.subplots(figsize=(10, 6))
            
            scatter = ax_scatter.scatter(df_analysis_clustered[x_var], 
                                       df_analysis_clustered[y_var], 
                                       c=df_analysis_clustered['CLUSTER'].astype('category').cat.codes,
                                       cmap='viridis', alpha=0.6, s=30)
            ax_scatter.set_xlabel(x_var)
            ax_scatter.set_ylabel(y_var)
            ax_scatter.set_title(f'Clusters: {x_var} vs {y_var}')
            ax_scatter.grid(True, alpha=0.3)
            
            # AÃ±adir leyenda de clusters
            plt.colorbar(scatter, ax=ax_scatter, label='Cluster')
            
            st.pyplot(fig_scatter)
            plt.close(fig_scatter)

    # ================================
    # ðŸ“ˆ ANÃLISIS DE CLUSTERS
    # ================================
    st.subheader("ðŸ“ˆ EstadÃ­sticas por Cluster")
    
    # EstadÃ­sticas descriptivas por cluster
    stats_clusters = df_analysis_clustered.groupby('CLUSTER').describe()
    st.dataframe(stats_clusters, use_container_width=True)
    
    # InterpretaciÃ³n de clusters
    st.markdown("#### ðŸ” InterpretaciÃ³n de Clusters")
    
    # Calcular promedios por cluster para interpretaciÃ³n
    promedios_cluster = df_analysis_clustered.groupby('CLUSTER').mean()
    
    # Mostrar caracterÃ­sticas de cada cluster
    for cluster_id in promedios_cluster.index:
        st.markdown(f"**Cluster {cluster_id}:**")
        cluster_data = promedios_cluster.loc[cluster_id]
        
        col_interp1, col_interp2 = st.columns(2)
        
        with col_interp1:
            # Encontrar variable con valor mÃ¡ximo
            var_max = cluster_data.idxmax()
            val_max = cluster_data.max()
            st.write(f"â€¢ **Mayor valor:** {var_max} = {val_max:.3f}")
        
        with col_interp2:
            # Encontrar variable con valor mÃ­nimo
            var_min = cluster_data.idxmin()
            val_min = cluster_data.min()
            st.write(f"â€¢ **Menor valor:** {var_min} = {val_min:.3f}")
        
        # InterpretaciÃ³n bÃ¡sica basada en los valores
        st.write("â€¢ **Posible interpretaciÃ³n:** ", end="")
        
        # AquÃ­ puedes agregar lÃ³gica de interpretaciÃ³n especÃ­fica para tus datos
        if 'GR' in variables_analisis:
            gr_val = cluster_data.get('GR', 0)
            if gr_val > promedios_cluster['GR'].mean():
                st.write("Posible zona lutÃ­tica")
            else:
                st.write("Posible zona arenosa")
        else:
            st.write("Analizar patrones especÃ­ficos de las curvas seleccionadas")

    # ================================
    # ðŸ’¾ EXPORTAR RESULTADOS
    # ================================
    st.subheader("ðŸ’¾ Exportar Resultados de InterpretaciÃ³n")
    
    if st.button("ðŸ“¥ Generar Reporte de InterpretaciÃ³n", use_container_width=True):
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Hoja de datos con clusters
            df_analysis_clustered.to_excel(writer, sheet_name='Datos_Clusters', index=False)
            
            # Hoja de estadÃ­sticas por cluster
            stats_clusters.to_excel(writer, sheet_name='Estadisticas_Clusters')
            
            # Hoja de interpretaciÃ³n
            interpretacion_data = []
            for cluster_id in promedios_cluster.index:
                cluster_row = {'Cluster': cluster_id}
                for var in variables_analisis:
                    cluster_row[var] = promedios_cluster.loc[cluster_id, var]
                cluster_row['Muestras'] = len(df_analysis_clustered[df_analysis_clustered['CLUSTER'] == cluster_id])
                interpretacion_data.append(cluster_row)
            
            interpretacion_df = pd.DataFrame(interpretacion_data)
            interpretacion_df.to_excel(writer, sheet_name='Interpretacion', index=False)
        
        output.seek(0)
        
        st.download_button(
            label="â¬‡ï¸ Descargar Reporte de InterpretaciÃ³n",
            data=output,
            file_name=f"interpretacion_avanzada_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
# ================================
# ðŸ“‹ MÃ“DULO 5: REPORTES AUTOMÃTICOS
# ================================
def modulo_reportes_automaticos():
    st.header("ðŸ“‹ GeneraciÃ³n de Reportes AutomÃ¡ticos")
    
    st.info("""
    **ðŸ“Š MÃ³dulo de Reportes AutomÃ¡ticos:**
    Genera reportes profesionales con anÃ¡lisis completo de los datos de pozo.
    Incluye estadÃ­sticas, grÃ¡ficos, interpretaciones y recomendaciones.
    """)
    
    # Cargar datos
    df = mostrar_cargador_datos(
        "Reportes AutomÃ¡ticos", 
        "Carga datos para generar reporte automÃ¡tico completo",
        permitir_multicarga=True
    )
    
    if df is None:
        return

    # ConfiguraciÃ³n del reporte
    st.subheader("âš™ï¸ ConfiguraciÃ³n del Reporte")
    
    col_report1, col_report2 = st.columns(2)
    
    with col_report1:
        nombre_pozo = st.text_input("Nombre del Pozo:", value="Pozo_Ejemplo")
        operadora = st.text_input("Operadora:", value="CompaÃ±Ã­a Ejemplo")
        formato_reporte = st.selectbox("Formato de salida:", options=["Excel Completo", "PDF Resumen"])
        
    with col_report2:
        incluir_estadisticas = st.checkbox("Incluir anÃ¡lisis estadÃ­stico", value=True)
        incluir_correlaciones = st.checkbox("Incluir matriz de correlaciones", value=True)
        incluir_graficos = st.checkbox("Incluir grÃ¡ficos principales", value=True)
        incluir_interpretacion = st.checkbox("Incluir interpretaciÃ³n", value=True)

    # SelecciÃ³n de variables para el reporte
    columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not columnas_numericas:
        st.error("âŒ No se encontraron columnas numÃ©ricas para el reporte")
        return

    variables_reporte = st.multiselect(
        "Variables a incluir en el reporte:",
        options=columnas_numericas,
        default=columnas_numericas,
        help="Selecciona las curvas que quieres incluir en el anÃ¡lisis del reporte"
    )

    if not variables_reporte:
        st.warning("âš ï¸ Por favor selecciona al menos una variable para el reporte")
        return

    # Generar reporte
    if st.button("ðŸš€ Generar Reporte AutomÃ¡tico", type="primary", use_container_width=True):
        with st.spinner("Generando reporte automÃ¡tico..."):
            
            # Crear reporte en Excel
            output = BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                
                # ===== HOJA 1: PORTADA Y METADATOS =====
                portada_data = {
                    'Campo': [nombre_pozo],
                    'Operadora': [operadora],
                    'Fecha de GeneraciÃ³n': [datetime.now().strftime('%Y-%m-%d %H:%M')],
                    'NÃºmero de Muestras': [len(df)],
                    'NÃºmero de Curvas': [len(variables_reporte)],
                    'Rango de Profundidad': [f"{df.select_dtypes(include=[np.number]).iloc[:, 0].min():.2f} - {df.select_dtypes(include=[np.number]).iloc[:, 0].max():.2f}"]
                }
                portada_df = pd.DataFrame(portada_data)
                portada_df.to_excel(writer, sheet_name='PORTADA', index=False)
                
                # ===== HOJA 2: DATOS ORIGINALES =====
                df[variables_reporte].to_excel(writer, sheet_name='DATOS_ORIGINALES', index=False)
                
                # ===== HOJA 3: ESTADÃSTICAS DESCRIPTIVAS =====
                if incluir_estadisticas:
                    stats_df = df[variables_reporte].describe().T
                    stats_df['Varianza'] = df[variables_reporte].var()
                    stats_df['AsimetrÃ­a'] = df[variables_reporte].skew()
                    stats_df['Curtosis'] = df[variables_reporte].kurtosis()
                    stats_df['Valores Nulos'] = df[variables_reporte].isnull().sum()
                    stats_df['% Nulos'] = (df[variables_reporte].isnull().sum() / len(df)) * 100
                    stats_df.to_excel(writer, sheet_name='ESTADISTICAS')
                
                # ===== HOJA 4: CORRELACIONES =====
                if incluir_correlaciones and len(variables_reporte) > 1:
                    corr_matrix = df[variables_reporte].corr()
                    corr_matrix.to_excel(writer, sheet_name='CORRELACIONES')
                
                # ===== HOJA 5: INTERPRETACIÃ“N =====
                if incluir_interpretacion:
                    interpretacion_data = []
                    
                    for variable in variables_reporte:
                        datos_var = df[variable].dropna()
                        if len(datos_var) > 0:
                            interpretacion_data.append({
                                'Variable': variable,
                                'Media': datos_var.mean(),
                                'Mediana': datos_var.median(),
                                'MÃ­nimo': datos_var.min(),
                                'MÃ¡ximo': datos_var.max(),
                                'Rango': datos_var.max() - datos_var.min(),
                                'InterpretaciÃ³n': generar_interpretacion(variable, datos_var)
                            })
                    
                    interpretacion_df = pd.DataFrame(interpretacion_data)
                    interpretacion_df.to_excel(writer, sheet_name='INTERPRETACION', index=False)
                
                # ===== HOJA 6: RECOMENDACIONES =====
                recomendaciones = generar_recomendaciones(df[variables_reporte])
                recomendaciones_df = pd.DataFrame(recomendaciones, columns=['Recomendaciones'])
                recomendaciones_df.to_excel(writer, sheet_name='RECOMENDACIONES', index=False)
            
            output.seek(0)
            
            # Descargar reporte
            st.success("âœ… Reporte generado exitosamente!")
            
            st.download_button(
                label="ðŸ“¥ Descargar Reporte Completo",
                data=output,
                file_name=f"reporte_{nombre_pozo}_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )

def generar_interpretacion(variable, data):
    """Genera interpretaciÃ³n automÃ¡tica basada en estadÃ­sticas de la variable"""
    mean_val = data.mean()
    std_val = data.std()
    skew_val = data.skew()
    
    interpretations = []
    
    # InterpretaciÃ³n basada en valores tÃ­picos de curvas de pozo
    if 'GR' in variable.upper():
        if mean_val < 50:
            interpretations.append("Zona potencialmente arenosa")
        elif mean_val > 100:
            interpretations.append("Zona potencialmente lutÃ­tica")
        else:
            interpretations.append("Zona intermedia arena-lutita")
    
    if 'RES' in variable.upper() or 'RT' in variable.upper():
        if mean_val > 100:
            interpretations.append("Alta resistividad - posible zona productora")
        elif mean_val < 10:
            interpretations.append("Baja resistividad - posible zona acuÃ­fera")
    
    if 'NPHI' in variable.upper() or 'PHI' in variable.upper():
        if mean_val > 0.15:
            interpretations.append("Buena porosidad - favorable")
        elif mean_val < 0.05:
            interpretations.append("Baja porosidad - desfavorable")
    
    if 'RHOB' in variable.upper():
        if mean_val < 2.3:
            interpretations.append("Baja densidad - posible porosidad")
        elif mean_val > 2.6:
            interpretations.append("Alta densidad - posible compactaciÃ³n")
    
    # InterpretaciÃ³n basada en distribuciÃ³n
    if abs(skew_val) > 1:
        interpretations.append("DistribuciÃ³n muy asimÃ©trica")
    elif abs(skew_val) > 0.5:
        interpretations.append("DistribuciÃ³n moderadamente asimÃ©trica")
    else:
        interpretations.append("DistribuciÃ³n relativamente simÃ©trica")
    
    return "; ".join(interpretations) if interpretations else "AnÃ¡lisis estÃ¡ndar - revisar valores especÃ­ficos"

def generar_recomendaciones(data):
    """Genera recomendaciones automÃ¡ticas basadas en el anÃ¡lisis de datos"""
    recommendations = []
    
    # AnÃ¡lisis de completitud de datos
    null_percentage = (data.isnull().sum() / len(data) * 100).max()
    if null_percentage > 50:
        recommendations.append("âš ï¸ ALTA PROPORCIÃ“N DE DATOS FALTANTES - Considerar adquisiciÃ³n adicional de datos")
    elif null_percentage > 20:
        recommendations.append("ðŸ“‹ DATOS INCOMPLETOS - Sugerir complementar con registros adicionales")
    
    # AnÃ¡lisis de variabilidad
    numeric_data = data.select_dtypes(include=[np.number])
    if not numeric_data.empty:
        cv_values = numeric_data.std() / numeric_data.mean()
        high_variability = (cv_values > 1).any()
        
        if high_variability:
            recommendations.append("ðŸ“Š ALTA VARIABILIDAD EN DATOS - Recomendable anÃ¡lisis detallado por zonas")
        else:
            recommendations.append("ðŸ“ˆ DATOS CONSISTENTES - Favorable para interpretaciÃ³n integrada")
    
    # Recomendaciones generales
    recommendations.extend([
        "âœ… VALIDAR DATOS CON NÃšCLEOS Y PRUEBAS DE POZO",
        "ðŸ” REALIZAR ANÃLISIS INTEGRADO CON DATOS SÃSMICOS",
        "ðŸ“ CONSIDERAR CONTEXTO GEOLÃ“GICO REGIONAL",
        "ðŸŽ¯ PRIORIZAR ZONAS CON MEJORES CARACTERÃSTICAS PETROFÃSICAS"
    ])
    
    return recommendations

# ================================
# ðŸ§  MÃ“DULO 6: MACHINE LEARNING AVANZADO
# ================================
def modulo_machine_learning():
    st.header("ðŸ§  Machine Learning para AnÃ¡lisis PetrofÃ­sico")
    
    st.markdown("""
    **ðŸ”® PredicciÃ³n inteligente de facies y propiedades de formaciÃ³n usando algoritmos de ML**
    
    **Funcionalidades:**
    â€¢ ClasificaciÃ³n automÃ¡tica de facies
    â€¢ PredicciÃ³n de propiedades petrofÃ­sicas
    â€¢ ClusterizaciÃ³n no supervisada
    â€¢ AnÃ¡lisis de componentes principales (PCA)
    â€¢ OptimizaciÃ³n de hiperparÃ¡metros
    """)
    
    # Cargar datos
    df = mostrar_cargador_datos(
        "Machine Learning", 
        "Carga datos con curvas de registros para entrenar modelos predictivos",
        permitir_multicarga=True
    )
    
    if df is None:
        return

    # SelecciÃ³n de modo de operaciÃ³n
    st.subheader("ðŸŽ¯ Modo de OperaciÃ³n")
    modo_ml = st.radio(
        "Selecciona el tipo de anÃ¡lisis:",
        ["ClasificaciÃ³n Supervisada", "ClusterizaciÃ³n No Supervisada", "PredicciÃ³n de Propiedades"],
        horizontal=True
    )

    # SelecciÃ³n de variables
    columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not columnas_numericas:
        st.error("âŒ No se encontraron columnas numÃ©ricas para anÃ¡lisis")
        return

    st.subheader("ðŸ“Š SelecciÃ³n de Variables")
    
    col_ml1, col_ml2 = st.columns(2)
    
    with col_ml1:
        features = st.multiselect(
            "Variables predictoras (Features):",
            options=columnas_numericas,
            default=columnas_numericas[:min(5, len(columnas_numericas))],
            help="Selecciona las curvas que usarÃ¡s para predecir"
        )
    
    with col_ml2:
        if modo_ml == "ClasificaciÃ³n Supervisada":
            # Buscar columna de facies o crear selector
            posibles_facies = [col for col in df.columns if any(x in col.upper() for x in ['FACIES', 'LITHO', 'CLASS', 'ZONE'])]
            if posibles_facies:
                target = st.selectbox("Variable objetivo (Facies):", options=posibles_facies)
            else:
                st.warning("No se encontrÃ³ columna de facies. Usa clusterizaciÃ³n no supervisada.")
                modo_ml = "ClusterizaciÃ³n No Supervisada"
        
        elif modo_ml == "PredicciÃ³n de Propiedades":
            target = st.selectbox(
                "Variable a predecir:",
                options=[col for col in columnas_numericas if col not in features]
            )

    if not features:
        st.warning("âš ï¸ Selecciona al menos una variable predictora")
        return

    # Preparar datos
    df_ml = df[features].copy().dropna()
    
    if df_ml.empty:
        st.error("âŒ No hay datos vÃ¡lidos despuÃ©s del preprocesamiento")
        return

    # ================================
    # ðŸŽ¯ CLASIFICACIÃ“N SUPERVISADA
    # ================================
    if modo_ml == "ClasificaciÃ³n Supervisada" and 'target' in locals():
        
        st.subheader("ðŸŽ¯ ConfiguraciÃ³n de ClasificaciÃ³n")
        
        # Preparar datos target
        y = df[target].copy()
        
        # Codificar labels si es necesario
        if y.dtype == 'object':
            le = LabelEncoder()
            y_encoded = le.fit_transform(y)
            st.info(f"Facies codificadas: {dict(zip(le.classes_, range(len(le.classes_))))}")
        else:
            y_encoded = y.values
        
        # Combinar features y target
        df_combined = df_ml.copy()
        df_combined = df_combined.loc[y.index]  # Alinear Ã­ndices
        df_combined['TARGET'] = y_encoded
        df_combined = df_combined.dropna()
        
        if df_combined.empty:
            st.error("âŒ No hay datos alineados entre features y target")
            return
        
        X = df_combined[features]
        y_final = df_combined['TARGET']
        
        # ConfiguraciÃ³n del modelo
        col_model1, col_model2 = st.columns(2)
        
        with col_model1:
            algoritmo = st.selectbox(
                "Algoritmo de clasificaciÃ³n:",
                ["Random Forest", "Gradient Boosting", "SVM", "Red Neuronal"]
            )
            
            test_size = st.slider("Porcentaje para test:", 0.1, 0.4, 0.2, 0.05)
        
        with col_model2:
            cv_folds = st.slider("Folds para validaciÃ³n cruzada:", 3, 10, 5)
            random_state = st.number_input("Semilla aleatoria:", 42, key="ml_rand")
        
        # Entrenar modelo
        if st.button("ðŸš€ Entrenar Modelo de ClasificaciÃ³n", type="primary"):
            with st.spinner("Entrenando modelo..."):
                
                # Dividir datos
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y_final, test_size=test_size, random_state=random_state, stratify=y_final
                )
                
                # Escalar features
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                # Seleccionar y configurar modelo
                if algoritmo == "Random Forest":
                    model = RandomForestClassifier(n_estimators=100, random_state=random_state)
                elif algoritmo == "Gradient Boosting":
                    model = GradientBoostingClassifier(random_state=random_state)
                elif algoritmo == "SVM":
                    model = SVC(probability=True, random_state=random_state)
                else:  # Red Neuronal
                    model = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=random_state)
                
                # Entrenar
                model.fit(X_train_scaled, y_train)
                
                # Predecir
                y_pred = model.predict(X_test_scaled)
                y_proba = model.predict_proba(X_test_scaled)
                
                # MÃ©tricas
                accuracy = accuracy_score(y_test, y_pred)
                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv_folds)
                
                # Mostrar resultados
                st.subheader("ðŸ“Š Resultados del Modelo")
                
                col_res1, col_res2, col_res3 = st.columns(3)
                
                with col_res1:
                    st.metric("Accuracy Test", f"{accuracy:.3f}")
                with col_res2:
                    st.metric("Accuracy CV Mean", f"{cv_scores.mean():.3f}")
                with col_res3:
                    st.metric("Accuracy CV Std", f"{cv_scores.std():.3f}")
                
                # Matriz de confusiÃ³n
                st.markdown("#### ðŸŽ¯ Matriz de ConfusiÃ³n")
                fig_cm, ax_cm = plt.subplots(figsize=(8, 6))
                cm = confusion_matrix(y_test, y_pred)
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm)
                ax_cm.set_xlabel('PredicciÃ³n')
                ax_cm.set_ylabel('Real')
                ax_cm.set_title('Matriz de ConfusiÃ³n')
                st.pyplot(fig_cm)
                
                # Reporte de clasificaciÃ³n
                st.markdown("#### ðŸ“‹ Reporte de ClasificaciÃ³n")
                report = classification_report(y_test, y_pred, output_dict=True)
                report_df = pd.DataFrame(report).transpose()
                st.dataframe(report_df.style.format("{:.3f}"), use_container_width=True)
                
                # Importancia de caracterÃ­sticas
                if hasattr(model, 'feature_importances_'):
                    st.markdown("#### ðŸ“ˆ Importancia de CaracterÃ­sticas")
                    feature_importance = pd.DataFrame({
                        'feature': features,
                        'importance': model.feature_importances_
                    }).sort_values('importance', ascending=False)
                    
                    fig_fi, ax_fi = plt.subplots(figsize=(10, 6))
                    sns.barplot(data=feature_importance, x='importance', y='feature', ax=ax_fi)
                    ax_fi.set_title('Importancia de CaracterÃ­sticas')
                    st.pyplot(fig_fi)
                
                # Guardar modelo en session state
                st.session_state.ml_model = model
                st.session_state.ml_scaler = scaler
                st.session_state.ml_features = features
                
                st.success("âœ… Modelo entrenado y guardado exitosamente!")

    # ================================
    # ðŸ” CLUSTERIZACIÃ“N NO SUPERVISADA
    # ================================
    elif modo_ml == "ClusterizaciÃ³n No Supervisada":
        
        st.subheader("ðŸ” ConfiguraciÃ³n de ClusterizaciÃ³n")
        
        col_clust1, col_clust2 = st.columns(2)
        
        with col_clust1:
            metodo_cluster = st.selectbox(
                "MÃ©todo de clusterizaciÃ³n:",
                ["K-Means", "DBSCAN", "Agrupamiento JerÃ¡rquico"]
            )
            
            if metodo_cluster == "K-Means":
                n_clusters = st.slider("NÃºmero de clusters:", 2, 10, 3)
            elif metodo_cluster == "DBSCAN":
                eps = st.slider("ParÃ¡metro EPS:", 0.1, 2.0, 0.5, 0.1)
                min_samples = st.slider("MÃ­nimo de muestras:", 2, 20, 5)
        
        with col_clust2:
            usar_pca = st.checkbox("Usar PCA para visualizaciÃ³n", value=True)
            random_state = st.number_input("Semilla aleatoria:", 42, key="cluster_rand")
        
        if st.button("ðŸŽ¯ Ejecutar ClusterizaciÃ³n", type="primary"):
            with st.spinner("Realizando clusterizaciÃ³n..."):
                
                # Escalar datos
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(df_ml)
                
                # Aplicar clusterizaciÃ³n
                if metodo_cluster == "K-Means":
                    clusterer = KMeans(n_clusters=n_clusters, random_state=random_state)
                elif metodo_cluster == "DBSCAN":
                    clusterer = DBSCAN(eps=eps, min_samples=min_samples)
                else:  # Agglomerative
                    from sklearn.cluster import AgglomerativeClustering
                    clusterer = AgglomerativeClustering(n_clusters=n_clusters)
                
                labels = clusterer.fit_predict(X_scaled)
                
                # AÃ±adir labels al DataFrame
                df_clustered = df_ml.copy()
                df_clustered['CLUSTER'] = labels
                df_clustered['CLUSTER'] = df_clustered['CLUSTER'].astype(str)
                
                # Resultados
                n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)
                st.success(f"âœ… ClusterizaciÃ³n completada: {n_clusters_found} clusters identificados")
                
                # VisualizaciÃ³n con PCA
                if usar_pca and len(features) > 1:
                    pca = PCA(n_components=2)
                    X_pca = pca.fit_transform(X_scaled)
                    
                    df_viz = pd.DataFrame({
                        'PC1': X_pca[:, 0],
                        'PC2': X_pca[:, 1],
                        'Cluster': labels.astype(str)
                    })
                    
                    fig_pca = px.scatter(
                        df_viz, x='PC1', y='PC2', color='Cluster',
                        title='VisualizaciÃ³n PCA de Clusters',
                        opacity=0.7
                    )
                    st.plotly_chart(fig_pca, use_container_width=True)
                    
                    # Varianza explicada
                    var_explained = pca.explained_variance_ratio_.sum()
                    st.info(f"**Varianza explicada por PCA:** {var_explained:.2%}")
                
                # EstadÃ­sticas por cluster
                st.markdown("#### ðŸ“Š EstadÃ­sticas por Cluster")
                stats_clusters = df_clustered.groupby('CLUSTER').describe()
                st.dataframe(stats_clusters, use_container_width=True)
                
                # InterpretaciÃ³n de clusters
                st.markdown("#### ðŸ” InterpretaciÃ³n de Clusters")
                
                promedios_cluster = df_clustered.groupby('CLUSTER').mean()
                
                for cluster_id in promedios_cluster.index:
                    if cluster_id != '-1':  # Excluir outliers de DBSCAN
                        st.markdown(f"**Cluster {cluster_id}:**")
                        cluster_data = promedios_cluster.loc[cluster_id]
                        
                        # CaracterÃ­sticas distintivas
                        var_max = cluster_data.idxmax()
                        var_min = cluster_data.idxmin()
                        
                        col_interp1, col_interp2 = st.columns(2)
                        with col_interp1:
                            st.write(f"â€¢ **Alto:** {var_max} = {cluster_data[var_max]:.3f}")
                        with col_interp2:
                            st.write(f"â€¢ **Bajo:** {var_min} = {cluster_data[var_min]:.3f}")
                        
                        # InterpretaciÃ³n petrofÃ­sica
                        interpretacion = interpretar_cluster_petrofisico(cluster_data, features)
                        st.write(f"â€¢ **InterpretaciÃ³n:** {interpretacion}")
                
                # Guardar resultados
                st.session_state.clustering_results = df_clustered
                st.session_state.clustering_scaler = scaler

def interpretar_cluster_petrofisico(cluster_data, features):
    """Interpreta un cluster basado en valores promedio de caracterÃ­sticas petrofÃ­sicas"""
    
    interpretaciones = []
    
    # AnÃ¡lisis basado en curvas tÃ­picas
    for feature, value in cluster_data.items():
        if 'GR' in feature.upper():
            if value > 100:
                interpretaciones.append("LutÃ­tico")
            elif value < 50:
                interpretaciones.append("Arenoso")
                
        elif 'RES' in feature.upper() or 'RT' in feature.upper():
            if value > 50:
                interpretaciones.append("Resistivo")
            elif value < 10:
                interpretaciones.append("Conductor")
                
        elif 'NPHI' in feature.upper() or 'PHI' in feature.upper():
            if value > 0.20:
                interpretaciones.append("Poroso")
            elif value < 0.10:
                interpretaciones.append("Compacto")
                
        elif 'RHOB' in feature.upper():
            if value < 2.3:
                interpretaciones.append("Baja densidad")
            elif value > 2.6:
                interpretaciones.append("Alta densidad")
    
    # Eliminar duplicados y unir
    interpretaciones_unicas = list(set(interpretaciones))
    
    if interpretaciones_unicas:
        return " | ".join(interpretaciones_unicas)
    else:
        return "Cluster con caracterÃ­sticas mixtas - anÃ¡lisis detallado requerido"
# ================================
# ðŸŽ›ï¸ FUNCIÃ“N PRINCIPAL ACTUALIZADA
# ================================
def main():
    """FunciÃ³n principal de la aplicaciÃ³n"""
    
    # Inicializar session state
    if 'df_actual' not in st.session_state:
        st.session_state.df_actual = None
    if 'multicarga_info' not in st.session_state:
        st.session_state.multicarga_info = None
    
    # Sidebar para navegaciÃ³n
    st.sidebar.title("ðŸ›¢ï¸ NavegaciÃ³n")
    st.sidebar.markdown("---")
    
    # SelecciÃ³n de mÃ³dulo
    modulo = st.sidebar.selectbox(
        "Selecciona el mÃ³dulo:",
        options=[
            "ðŸ  Inicio",
            "ðŸ“Š VisualizaciÃ³n BÃ¡sica", 
            "ðŸ“ˆ AnÃ¡lisis EstadÃ­stico",
            "ðŸ›¢ï¸ AnÃ¡lisis PetrofÃ­sico",
            "ðŸ” InterpretaciÃ³n Avanzada",
            "ðŸ“‹ Reportes AutomÃ¡ticos",
            "ðŸ§  Machine Learning Avanzado"
        ]
    )
    
    # InformaciÃ³n en sidebar
    st.sidebar.markdown("---")
    st.sidebar.subheader("ðŸ”„ Novedades")
    st.sidebar.success("""
    **âœ¨ Nuevas caracterÃ­sticas:**
    â€¢ **Lasio integrado** para mejor procesamiento LAS
    â€¢ **Multicarga de archivos** 
    â€¢ **CombinaciÃ³n automÃ¡tica** de datos
    â€¢ **Metadatos completos** de curvas
    """)
    
    # NavegaciÃ³n a mÃ³dulos
    if modulo == "ðŸ  Inicio":
        st.header("ðŸ  Bienvenido al Sistema Avanzado de AnÃ¡lisis PetrofÃ­sico")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            ### ðŸš€ **CaracterÃ­sticas Principales**
            
            **ðŸ“ Carga Inteligente:**
            â€¢ **Lasio integrado** - Procesamiento profesional de archivos LAS
            â€¢ **Multicarga** - Combina mÃºltiples archivos automÃ¡ticamente
            â€¢ **DetecciÃ³n automÃ¡tica** de formatos
            â€¢ **Metadatos completos** de curvas y pozos
            
            **ðŸ“Š MÃ³dulos de AnÃ¡lisis:**
            â€¢ **VisualizaciÃ³n avanzada** - GrÃ¡ficos profesionales de registros
            â€¢ **AnÃ¡lisis estadÃ­stico** - Correlaciones y distribuciones
            â€¢ **AnÃ¡lisis petrofÃ­sico** - Porosidad, saturaciÃ³n, Vshale
            â€¢ **InterpretaciÃ³n avanzada** - ClusterizaciÃ³n y facies
            â€¢ **Reportes automÃ¡ticos** - GeneraciÃ³n profesional
            â€¢ **Machine Learning** - PredicciÃ³n inteligente
            """)
        
        with col2:
            st.markdown("""
            ### ðŸŽ¯ **Novedades Implementadas**
            
            **ðŸ”„ Multicarga de Archivos:**
            â€¢ Carga mÃºltiples archivos LAS/Excel simultÃ¡neamente
            â€¢ CombinaciÃ³n automÃ¡tica de datos
            â€¢ PreservaciÃ³n de metadatos
            â€¢ Filtrado por archivo individual
            
            **ðŸ”§ Lasio Integrado:**
            â€¢ Procesamiento robusto de archivos LAS
            â€¢ InformaciÃ³n completa de curvas (unidades, descripciones)
            â€¢ Metadatos estructurados del pozo
            â€¢ Manejo de valores nulos especÃ­ficos
            
            **ðŸ’¾ ExportaciÃ³n Profesional:**
            â€¢ Excel con mÃºltiples hojas
            â€¢ Configuraciones guardadas
            â€¢ Reportes estadÃ­sticos completos
            """)
        
        # InformaciÃ³n de instalaciÃ³n
        st.markdown("---")
        st.subheader("ðŸ”§ InstalaciÃ³n y Requisitos")
        
        col_inst1, col_inst2 = st.columns(2)
        
        with col_inst1:
            st.markdown("""
            **ðŸ“¦ Para mejor experiencia, instala:**
            ```bash
            pip install lasio
            ```
            
            **âœ… CaracterÃ­sticas con Lasio:**
            â€¢ Procesamiento profesional de LAS
            â€¢ Metadatos estructurados
            â€¢ Unidades y descripciones
            â€¢ Compatibilidad con estÃ¡ndares
            """)
        
        with col_inst2:
            st.markdown("""
            **ðŸ› ï¸ Funcionalidades base:**
            â€¢ Procesamiento manual de LAS
            â€¢ Carga de Excel
            â€¢ VisualizaciÃ³n bÃ¡sica
            â€¢ AnÃ¡lisis estadÃ­stico
            â€¢ Multicarga de archivos
            â€¢ Todos los mÃ³dulos de anÃ¡lisis
            """)
        
    elif modulo == "ðŸ“Š VisualizaciÃ³n BÃ¡sica":
        modulo_visualizacion_basica()
        
    elif modulo == "ðŸ“ˆ AnÃ¡lisis EstadÃ­stico":
        modulo_analisis_estadistico()
        
    elif modulo == "ðŸ›¢ï¸ AnÃ¡lisis PetrofÃ­sico":
        modulo_analisis_petrofisico()
        
    elif modulo == "ðŸ” InterpretaciÃ³n Avanzada":
        modulo_interpretacion_avanzada()
        
    elif modulo == "ðŸ“‹ Reportes AutomÃ¡ticos":
        modulo_reportes_automaticos()
        
    elif modulo == "ðŸ§  Machine Learning Avanzado":
        modulo_machine_learning()

# Ejecutar la aplicaciÃ³n
if __name__ == "__main__":
    main()